{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248291c5",
   "metadata": {},
   "source": [
    "# Convert FFB6D to TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db67a3",
   "metadata": {},
   "source": [
    "After cloning this repositiory please follow the instructions on compiling apex, normalspeed and RandLA. Train your model with your dataset obtained from RasterTriangle, PVNetRendering, BopToolkit & BlenderProc.\n",
    "After you obtained a FFB6D model, you can follow these steps to convert it into Torchschript:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23846",
   "metadata": {},
   "source": [
    "## Prepare your Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ec0e2",
   "metadata": {},
   "source": [
    "#### Requirements:\n",
    "\n",
    "- active conda env with python >= 3.6 \n",
    "- pytorch (+ cuda toolkit version >= 10.2)\n",
    "- torchvision\n",
    "- torchaudio\n",
    "- pyyaml\n",
    "- coremltools\n",
    "- numpy as np\n",
    "- Pillow\n",
    "- Pickle"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "id": "407c0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
<<<<<<< Updated upstream
    "import torch.nn.functional as F\n",
    "from models.cnn.pspnet import PSPNet\n",
    "import models.pytorch_utils as pt_utils\n",
    "from models.RandLA.RandLANet import Network as RandLANet"
=======
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b3b55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if Cuda is installed properly\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea821958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Sun_Mar_21_19:15:46_PDT_2021\r\n",
      "Cuda compilation tools, release 11.3, V11.3.58\r\n",
      "Build cuda_11.3.r11.3/compiler.29745058_0\r\n"
     ]
    }
   ],
   "source": [
    "#If \"True\": ignore this and next command\n",
    "#If \"False\": Check if cuda is installed properly with\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a387963",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-b9e7614b51c1>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-b9e7614b51c1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# If not: Install cuda toolkit & CudNN driver\n",
    "# If version shows: run this command with an active conda environment:\n",
    "cuda_version = 11.3 # your installed cuda version\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "187d8846",
   "metadata": {},
   "source": [
    "## Add PSPNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super(PSPModule, self).__init__()\n",
    "        self.stages = []\n",
    "        self.stages = nn.ModuleList(\n",
    "            [self._make_stage(features, size) for size in sizes]\n",
    "        )\n",
    "        self.bottleneck = nn.Conv2d(\n",
    "            features * (len(sizes) + 1), out_features, kernel_size=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)\n",
    "        priors = [\n",
    "            F.upsample(input=stage(feats), size=(h, w), mode='bilinear')\n",
    "            for stage in self.stages\n",
    "        ] + [feats]\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)\n",
    "\n",
    "\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PSPUpsample, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Modified_PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "                 deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "                 ):\n",
    "        super(Modified_PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "            deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "    ):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            # nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da0631",
   "metadata": {},
   "source": [
    "## Add RandLANet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.fc0 = pt_utils.Conv1d(config.in_c, 8, kernel_size=1, bn=True)\n",
    "\n",
    "        self.dilated_res_blocks = nn.ModuleList()\n",
    "        d_in = 8\n",
    "        for i in range(self.config.num_layers):\n",
    "            d_out = self.config.d_out[i]\n",
    "            self.dilated_res_blocks.append(Dilated_res_block(d_in, d_out))\n",
    "            d_in = 2 * d_out\n",
    "\n",
    "        d_out = d_in\n",
    "        self.decoder_0 = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        for j in range(self.config.num_layers):\n",
    "            if j < 3:\n",
    "                d_in = d_out + 2 * self.config.d_out[-j-2]\n",
    "                d_out = 2 * self.config.d_out[-j-2]\n",
    "            else:\n",
    "                d_in = 4 * self.config.d_out[-4]\n",
    "                d_out = 2 * self.config.d_out[-4]\n",
    "            self.decoder_blocks.append(pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True))\n",
    "\n",
    "        self.fc1 = pt_utils.Conv2d(d_out, 64, kernel_size=(1,1), bn=True)\n",
    "        self.fc2 = pt_utils.Conv2d(64, 32, kernel_size=(1,1), bn=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = pt_utils.Conv2d(32, self.config.num_classes, kernel_size=(1,1), bn=False, activation=None)\n",
    "\n",
    "    def forward(self, end_points):\n",
    "\n",
    "        features = end_points['features']  # Batch*channel*npoints\n",
    "        features = self.fc0(features)\n",
    "\n",
    "        features = features.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###########################Encoder############################\n",
    "        f_encoder_list = []\n",
    "        for i in range(self.config.num_layers):\n",
    "            f_encoder_i = self.dilated_res_blocks[i](\n",
    "                features, end_points['xyz'][i], end_points['neigh_idx'][i]\n",
    "            )\n",
    "\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, end_points['sub_idx'][i])\n",
    "            features = f_sampled_i\n",
    "            print(\"encoder%d:\"%i, features.size())\n",
    "            if i == 0:\n",
    "                f_encoder_list.append(f_encoder_i)\n",
    "            f_encoder_list.append(f_sampled_i)\n",
    "        # ###########################Encoder############################\n",
    "\n",
    "        features = self.decoder_0(f_encoder_list[-1])\n",
    "\n",
    "        # ###########################Decoder############################\n",
    "        f_decoder_list = []\n",
    "        for j in range(self.config.num_layers):\n",
    "            f_interp_i = self.nearest_interpolation(features, end_points['interp_idx'][-j - 1])\n",
    "            f_decoder_i = self.decoder_blocks[j](torch.cat([f_encoder_list[-j - 2], f_interp_i], dim=1))\n",
    "\n",
    "            features = f_decoder_i\n",
    "            print(\"decoder%d:\"%j, features.size())\n",
    "            f_decoder_list.append(f_decoder_i)\n",
    "        # ###########################Decoder############################\n",
    "\n",
    "        features = self.fc1(features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc3(features)\n",
    "        f_out = features.squeeze(3)\n",
    "\n",
    "        end_points['logits'] = f_out\n",
    "        return end_points\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1))\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, C, npoints] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, c, up_num_points, 1] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(feature, 2, interp_idx.unsqueeze(1).repeat(1,feature.shape[1],1))\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "\n",
    "\n",
    "def compute_acc(end_points):\n",
    "\n",
    "    logits = end_points['valid_logits']\n",
    "    labels = end_points['valid_labels']\n",
    "    logits = logits.max(dim=1)[1]\n",
    "    acc = (logits == labels).sum().float() / float(labels.shape[0])\n",
    "    end_points['acc'] = acc\n",
    "    return acc, end_points\n",
    "\n",
    "\n",
    "class IoUCalculator:\n",
    "    def __init__(self, cfg):\n",
    "        self.gt_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.true_positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def add_data(self, end_points):\n",
    "        logits = end_points['valid_logits']\n",
    "        labels = end_points['valid_labels']\n",
    "        pred = logits.max(dim=1)[1]\n",
    "        pred_valid = pred.detach().cpu().numpy()\n",
    "        labels_valid = labels.detach().cpu().numpy()\n",
    "\n",
    "        val_total_correct = 0\n",
    "        val_total_seen = 0\n",
    "\n",
    "        correct = np.sum(pred_valid == labels_valid)\n",
    "        val_total_correct += correct\n",
    "        val_total_seen += len(labels_valid)\n",
    "\n",
    "        conf_matrix = confusion_matrix(labels_valid, pred_valid, np.arange(0, self.cfg.num_classes, 1))\n",
    "        self.gt_classes += np.sum(conf_matrix, axis=1)\n",
    "        self.positive_classes += np.sum(conf_matrix, axis=0)\n",
    "        self.true_positive_classes += np.diagonal(conf_matrix)\n",
    "\n",
    "    def compute_iou(self):\n",
    "        iou_list = []\n",
    "        for n in range(0, self.cfg.num_classes, 1):\n",
    "            if float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n]) != 0:\n",
    "                iou = self.true_positive_classes[n] / float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n])\n",
    "                iou_list.append(iou)\n",
    "            else:\n",
    "                iou_list.append(0.0)\n",
    "        mean_iou = sum(iou_list) / float(self.cfg.num_classes)\n",
    "        return mean_iou, iou_list\n",
    "\n",
    "\n",
    "\n",
    "class Dilated_res_block(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = pt_utils.Conv2d(d_in, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.lfa = Building_block(d_out)\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out, d_out*2, kernel_size=(1, 1), bn=True, activation=None)\n",
    "        self.shortcut = pt_utils.Conv2d(d_in, d_out*2, kernel_size=(1,1), bn=True, activation=None)\n",
    "\n",
    "    def forward(self, feature, xyz, neigh_idx):\n",
    "        f_pc = self.mlp1(feature)  # Batch*channel*npoints*1\n",
    "        f_pc = self.lfa(xyz, f_pc, neigh_idx)  # Batch*d_out*npoints*1\n",
    "        f_pc = self.mlp2(f_pc)\n",
    "        shortcut = self.shortcut(feature)\n",
    "        return F.leaky_relu(f_pc+shortcut, negative_slope=0.2)\n",
    "\n",
    "\n",
    "class Building_block(nn.Module):\n",
    "    def __init__(self, d_out):  #  d_in = d_out//2\n",
    "        super().__init__()\n",
    "        self.mlp1 = pt_utils.Conv2d(10, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.att_pooling_1 = Att_pooling(d_out, d_out//2)\n",
    "\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out//2, d_out//2, kernel_size=(1, 1), bn=True)\n",
    "        self.att_pooling_2 = Att_pooling(d_out, d_out)\n",
    "\n",
    "    def forward(self, xyz, feature, neigh_idx):  # feature: Batch*channel*npoints*1\n",
    "        f_xyz = self.relative_pos_encoding(xyz, neigh_idx)  # batch*npoint*nsamples*10\n",
    "        f_xyz = f_xyz.permute((0, 3, 1, 2)).contiguous()  # batch*10*npoint*nsamples\n",
    "        f_xyz = self.mlp1(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            feature.squeeze(-1).permute((0, 2, 1)).contiguous(),neigh_idx\n",
    "        )  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_1(f_concat)  # Batch*channel*npoints*1\n",
    "\n",
    "        f_xyz = self.mlp2(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            f_pc_agg.squeeze(-1).permute((0, 2, 1)).contiguous(), neigh_idx\n",
    "        ).contiguous()  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_2(f_concat)\n",
    "        return f_pc_agg\n",
    "\n",
    "    def relative_pos_encoding(self, xyz, neigh_idx):\n",
    "        neighbor_xyz = self.gather_neighbour(xyz, neigh_idx)  # batch*npoint*nsamples*3\n",
    "\n",
    "        xyz_tile = xyz.unsqueeze(2).repeat(1, 1, neigh_idx.shape[-1], 1)  # batch*npoint*nsamples*3\n",
    "        relative_xyz = xyz_tile - neighbor_xyz  # batch*npoint*nsamples*3\n",
    "        relative_dis = torch.sqrt(torch.sum(torch.pow(relative_xyz, 2), dim=-1, keepdim=True))  # batch*npoint*nsamples*1\n",
    "        relative_feature = torch.cat([relative_dis, relative_xyz, xyz_tile, neighbor_xyz], dim=-1)  # batch*npoint*nsamples*10\n",
    "        return relative_feature\n",
    "\n",
    "    @staticmethod\n",
    "    def gather_neighbour(pc, neighbor_idx):  # pc: batch*npoint*channel\n",
    "        # gather the coordinates or features of neighboring points\n",
    "        batch_size = pc.shape[0]\n",
    "        num_points = pc.shape[1]\n",
    "        d = pc.shape[2]\n",
    "        index_input = neighbor_idx.reshape(batch_size, -1)\n",
    "        features = torch.gather(pc, 1, index_input.unsqueeze(-1).repeat(1, 1, pc.shape[2])).contiguous()\n",
    "        features = features.reshape(batch_size, num_points, neighbor_idx.shape[-1], d)  # batch*npoint*nsamples*channel\n",
    "        return features\n",
    "\n",
    "\n",
    "class Att_pooling(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Conv2d(d_in, d_in, (1, 1), bias=False)\n",
    "        self.mlp = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "    def forward(self, feature_set):\n",
    "\n",
    "        att_activation = self.fc(feature_set)\n",
    "        att_scores = F.softmax(att_activation, dim=3)\n",
    "        f_agg = feature_set * att_scores\n",
    "        f_agg = torch.sum(f_agg, dim=3, keepdim=True)\n",
    "        f_agg = self.mlp(f_agg)\n",
    "        return f_agg\n",
    "\n",
    "\n",
    "def compute_loss(end_points, cfg):\n",
    "\n",
    "    logits = end_points['logits']\n",
    "    labels = end_points['labels']\n",
    "\n",
    "    logits = logits.transpose(1, 2).reshape(-1, cfg.num_classes)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    # Boolean mask of points that should be ignored\n",
    "    ignored_bool = labels == 0\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        ignored_bool = ignored_bool | (labels == ign_label)\n",
    "\n",
    "    # Collect logits and labels that are not ignored\n",
    "    valid_idx = ignored_bool == 0\n",
    "    valid_logits = logits[valid_idx, :]\n",
    "    valid_labels_init = labels[valid_idx]\n",
    "\n",
    "    # Reduce label values in the range of logit shape\n",
    "    reducing_list = torch.range(0, cfg.num_classes).long().cuda()\n",
    "    inserted_value = torch.zeros((1,)).long().cuda()\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        reducing_list = torch.cat([reducing_list[:ign_label], inserted_value, reducing_list[ign_label:]], 0)\n",
    "    valid_labels = torch.gather(reducing_list, 0, valid_labels_init)\n",
    "    loss = get_loss(valid_logits, valid_labels, cfg.class_weights)\n",
    "    end_points['valid_logits'], end_points['valid_labels'] = valid_logits, valid_labels\n",
    "    end_points['loss'] = loss\n",
    "    return loss, end_points\n",
    "\n",
    "\n",
    "def get_loss(logits, labels, pre_cal_weights):\n",
    "    # calculate the weighted cross entropy according to the inverse frequency\n",
    "    class_weights = torch.from_numpy(pre_cal_weights).float().cuda()\n",
    "    # one_hot_labels = F.one_hot(labels, self.config.num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='none')\n",
    "    output_loss = criterion(logits, labels)\n",
    "    output_loss = output_loss.mean()\n",
    "    return output_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
=======
>>>>>>> Stashed changes
   "id": "5a53764e",
   "metadata": {},
   "source": [
    "## FFB6D\n",
    "\n",
    "Let's look at the Network we want toi convert to TorchScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c914164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.cnn.pspnet import PSPNet\n",
    "import models.pytorch_utils as pt_utils\n",
    "from models.RandLA.RandLANet import Network as RandLANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "}\n",
    "\n",
    "\n",
    "class FFB6D(nn.Module):\n",
    "    def __init__(self, n_classes, n_pts, rndla_cfg, n_kps=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # ######################## prepare stages#########################\n",
    "        self.n_cls = n_classes\n",
    "        self.n_pts = n_pts\n",
    "        self.n_kps = n_kps\n",
    "        cnn = psp_models['resnet34'.lower()]()\n",
    "\n",
    "        rndla = RandLANet(rndla_cfg)\n",
    "\n",
    "        self.cnn_pre_stages = nn.Sequential (\n",
    "            cnn.feats.conv1,  # stride = 2, [bs, c, 240, 320]\n",
    "            cnn.feats.bn1, cnn.feats.relu,\n",
    "            cnn.feats.maxpool  # stride = 2, [bs, 64, 120, 160]\n",
    "        )\n",
    "        self.rndla_pre_stages = rndla.fc0\n",
    "\n",
    "        # ####################### downsample stages#######################\n",
    "        self.cnn_ds_stages = nn.ModuleList([\n",
    "            cnn.feats.layer1,    # stride = 1, [bs, 64, 120, 160]\n",
    "            cnn.feats.layer2,    # stride = 2, [bs, 128, 60, 80]\n",
    "            # stride = 1, [bs, 128, 60, 80]\n",
    "            nn.Sequential(cnn.feats.layer3, cnn.feats.layer4),\n",
    "            nn.Sequential(cnn.psp, cnn.drop_1)   # [bs, 1024, 60, 80]\n",
    "        ])\n",
    "        self.ds_sr = [4, 8, 8, 8]\n",
    "\n",
    "        self.rndla_ds_stages = rndla.dilated_res_blocks\n",
    "\n",
    "        self.ds_rgb_oc = [64, 128, 512, 1024]\n",
    "        self.ds_rndla_oc = [item * 2 for item in rndla_cfg.d_out]\n",
    "        self.ds_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.ds_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i], self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i]*2, self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.ds_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i], self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i]*2, self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ###################### upsample stages #############################\n",
    "        self.cnn_up_stages = nn.ModuleList([\n",
    "            nn.Sequential(cnn.up_1, cnn.drop_2),  # [bs, 256, 120, 160]\n",
    "            nn.Sequential(cnn.up_2, cnn.drop_2),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.final),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.up_3, cnn.final)  # [bs, 64, 480, 640]\n",
    "        ])\n",
    "        self.up_rgb_oc = [256, 64, 64]\n",
    "        self.up_rndla_oc = []\n",
    "        for j in range(rndla_cfg.num_layers):\n",
    "            if j < 3:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[-j-2])\n",
    "            else:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[0])\n",
    "\n",
    "        self.rndla_up_stages = rndla.decoder_blocks\n",
    "\n",
    "        n_fuse_layer = 3\n",
    "        self.up_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(n_fuse_layer):\n",
    "            self.up_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i], self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i]*2, self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.up_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i], self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i]*2, self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ####################### prediction headers #############################\n",
    "        # We use 3D keypoint prediction header for pose estimation following PVN3D\n",
    "        # You can use different prediction headers for different downstream tasks.\n",
    "\n",
    "        self.rgbd_seg_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1] + self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_classes, activation=None)\n",
    "        )\n",
    "\n",
    "        self.ctr_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(3, activation=None)\n",
    "        )\n",
    "\n",
    "        self.kp_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_kps*3, activation=None)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        if len(feature.size()) > 3:\n",
    "            feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(\n",
    "            feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, up_num_points, d] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(\n",
    "            feature, 2, interp_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "    def _break_up_pc(self, pc):\n",
    "        xyz = pc[:, :3, :].transpose(1, 2).contiguous()\n",
    "        features = (\n",
    "            pc[:, 3:, :].contiguous() if pc.size(1) > 3 else None\n",
    "        )\n",
    "        return xyz, features\n",
    "\n",
    "    def forward(\n",
    "        self, inputs, end_points=None, scale=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        inputs: dict of :\n",
    "            rgb         : FloatTensor [bs, 3, h, w]\n",
    "            dpt_nrm     : FloatTensor [bs, 6, h, w], 3c xyz in meter + 3c normal map\n",
    "            cld_rgb_nrm : FloatTensor [bs, 9, npts]\n",
    "            choose      : LongTensor [bs, 1, npts]\n",
    "            xmap, ymap: [bs, h, w]\n",
    "            K:          [bs, 3, 3]\n",
    "        Returns:\n",
    "            end_points:\n",
    "        \"\"\"\n",
    "        # ###################### prepare stages #############################\n",
    "        if not end_points:\n",
    "            end_points = {}\n",
    "        # ResNet pre + layer1 + layer2\n",
    "        rgb_emb = self.cnn_pre_stages(inputs['rgb'])  # stride = 2, [bs, c, 240, 320]\n",
    "        # rndla pre\n",
    "        xyz, p_emb = self._break_up_pc(inputs['cld_rgb_nrm'])\n",
    "        p_emb = inputs['cld_rgb_nrm']\n",
    "        p_emb = self.rndla_pre_stages(p_emb)\n",
    "        p_emb = p_emb.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###################### encoding stages #############################\n",
    "        ds_emb = []\n",
    "        for i_ds in range(4):\n",
    "            # encode rgb downsampled feature\n",
    "            rgb_emb0 = self.cnn_ds_stages[i_ds](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # encode point cloud downsampled feature\n",
    "            f_encoder_i = self.rndla_ds_stages[i_ds](\n",
    "                p_emb, inputs['cld_xyz%d' % i_ds], inputs['cld_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, inputs['cld_sub_idx%d' % i_ds])\n",
    "            p_emb0 = f_sampled_i\n",
    "            if i_ds == 0:\n",
    "                ds_emb.append(f_encoder_i)\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.ds_fuse_p2r_pre_layers[i_ds](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_ds_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.ds_fuse_p2r_fuse_layers[i_ds](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr, 1), inputs['r2p_ds_nei_idx%d' % i_ds]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.ds_fuse_r2p_pre_layers[i_ds](r2p_emb)\n",
    "            p_emb = self.ds_fuse_r2p_fuse_layers[i_ds](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "            ds_emb.append(p_emb)\n",
    "\n",
    "        # ###################### decoding stages #############################\n",
    "        n_up_layers = len(self.rndla_up_stages)\n",
    "        for i_up in range(n_up_layers-1):\n",
    "            # decode rgb upsampled feature\n",
    "            rgb_emb0 = self.cnn_up_stages[i_up](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # decode point cloud upsampled feature\n",
    "            f_interp_i = self.nearest_interpolation(\n",
    "                p_emb, inputs['cld_interp_idx%d' % (n_up_layers-i_up-1)]\n",
    "            )\n",
    "            f_decoder_i = self.rndla_up_stages[i_up](\n",
    "                torch.cat([ds_emb[-i_up - 2], f_interp_i], dim=1)\n",
    "            )\n",
    "            p_emb0 = f_decoder_i\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.up_fuse_p2r_pre_layers[i_up](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_up_nei_idx%d' % i_up]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.up_fuse_p2r_fuse_layers[i_up](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr), inputs['r2p_up_nei_idx%d' % i_up]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.up_fuse_r2p_pre_layers[i_up](r2p_emb)\n",
    "            p_emb = self.up_fuse_r2p_fuse_layers[i_up](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "\n",
    "        # final upsample layers:\n",
    "        rgb_emb = self.cnn_up_stages[n_up_layers-1](rgb_emb)\n",
    "        f_interp_i = self.nearest_interpolation(\n",
    "            p_emb, inputs['cld_interp_idx%d' % (0)]\n",
    "        )\n",
    "        p_emb = self.rndla_up_stages[n_up_layers-1](\n",
    "            torch.cat([ds_emb[0], f_interp_i], dim=1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        bs, di, _, _ = rgb_emb.size()\n",
    "        rgb_emb_c = rgb_emb.view(bs, di, -1)\n",
    "        choose_emb = inputs['choose'].repeat(1, di, 1)\n",
    "        rgb_emb_c = torch.gather(rgb_emb_c, 2, choose_emb).contiguous()\n",
    "\n",
    "        # Use DenseFusion in final layer, which will hurt performance due to overfitting\n",
    "        # rgbd_emb = self.fusion_layer(rgb_emb, pcld_emb)\n",
    "\n",
    "        # Use simple concatenation. Good enough for fully fused RGBD feature.\n",
    "        rgbd_emb = torch.cat([rgb_emb_c, p_emb], dim=1)\n",
    "\n",
    "        # ###################### prediction stages #############################\n",
    "        rgbd_segs = self.rgbd_seg_layer(rgbd_emb)\n",
    "        pred_kp_ofs = self.kp_ofst_layer(rgbd_emb)\n",
    "        pred_ctr_ofs = self.ctr_ofst_layer(rgbd_emb)\n",
    "\n",
    "        pred_kp_ofs = pred_kp_ofs.view(\n",
    "            bs, self.n_kps, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "        pred_ctr_ofs = pred_ctr_ofs.view(\n",
    "            bs, 1, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        # return rgbd_seg, pred_kp_of, pred_ctr_of\n",
    "        end_points['pred_rgbd_segs'] = rgbd_segs\n",
    "        end_points['pred_kp_ofs'] = pred_kp_ofs\n",
    "        end_points['pred_ctr_ofs'] = pred_ctr_ofs\n",
    "\n",
    "        return end_points\n",
    "\n",
    "\n",
    "# Copy from PVN3D: https://github.com/ethnhe/PVN3D\n",
    "class DenseFusion(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(DenseFusion, self).__init__()\n",
    "        self.conv2_rgb = torch.nn.Conv1d(64, 256, 1)\n",
    "        self.conv2_cld = torch.nn.Conv1d(32, 256, 1)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv1d(96, 512, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        self.ap1 = torch.nn.AvgPool1d(num_points)\n",
    "\n",
    "    def forward(self, rgb_emb, cld_emb):\n",
    "        bs, _, n_pts = cld_emb.size()\n",
    "        feat_1 = torch.cat((rgb_emb, cld_emb), dim=1)\n",
    "        rgb = F.relu(self.conv2_rgb(rgb_emb))\n",
    "        cld = F.relu(self.conv2_cld(cld_emb))\n",
    "\n",
    "        feat_2 = torch.cat((rgb, cld), dim=1)\n",
    "\n",
    "        rgbd = F.relu(self.conv3(feat_1))\n",
    "        rgbd = F.relu(self.conv4(rgbd))\n",
    "\n",
    "        ap_x = self.ap1(rgbd)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "        return torch.cat([feat_1, feat_2, ap_x], 1)  # 96+ 512 + 1024 = 1632\n",
    "\n",
    "\n",
    "def main():\n",
    "    from common import ConfigRandLA\n",
    "    rndla_cfg = ConfigRandLA\n",
    "\n",
    "    n_cls = 3\n",
    "    model = FFB6D(n_cls, rndla_cfg.num_points, rndla_cfg)\n",
    "    print(model)\n",
    "    #ffb6d_scripted =torch.jit.script(model)\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"model parameters:\", sum(param.numel() for param in model.parameters())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38d721",
   "metadata": {},
   "source": [
    "Provide input from example data to trace the flow:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "id": "0681c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools\n",
<<<<<<< Updated upstream
    "import tarfile"
=======
    "import tarfile\n",
    "from torch.jit import script, trace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3a41b",
   "metadata": {},
   "source": [
    "If your Pytorch version is <1.10.2 it might not load your model correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfd60ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Pytorch Verison\n",
    "torch.__version__"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "13b7d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls='vase'\n",
    "filename = train_log/custom/checkpoints/${cls}/FFB6D_${cls}_best.pth.tar\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(filename)\n",
    "except Exception:\n",
    "    checkpoint = pkl.load(open(filename, \"rb\"))\n",
    "with Image.open(os.path.join(self.cls_root, \"depth/{}.png\".format(item_name))) as di:\n",
    "    dpt_mm = np.array(di)\n",
    "example_input = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb9ab1",
   "metadata": {},
   "source": [
    "Load your pretrained model, set it to evaluation mode and provide sample input. (Normally gets passed into forward() method)"
=======
   "execution_count": 11,
   "id": "8a20d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = \"vase\"\n",
    "model_pth = \"model/FFB6D_{}_best.pth.tar\".format(cls)\n",
    "\n",
    "# Create an instance of your pretrained model.\n",
    "try:\n",
    "    model = torch.load(model_pth)\n",
    "except Exception:\n",
    "    model = pkl.load(open(model_pth, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148ccb5",
   "metadata": {},
   "source": [
    "Determine the shape of the forward() layer and create random sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ea84674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1920, 1440])\n",
      "torch.float32\n",
      "torch.Size([2, 6, 1920, 1440])\n",
      "torch.float32\n",
      "torch.Size([2, 9, 8])\n",
      "torch.float32\n",
      "torch.Size([2, 1, 8])\n",
      "torch.int64\n",
      "torch.Size([2, 1920, 1440])\n",
      "torch.float32\n",
      "torch.Size([2, 3, 3])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    " \"\"\"\n",
    "Params:\n",
    "inputs: dict of :\n",
    "    rgb         : FloatTensor [bs, 3, h, w]\n",
    "    dpt_nrm     : FloatTensor [bs, 6, h, w], 3c xyz in meter + 3c normal map\n",
    "    cld_rgb_nrm : FloatTensor [bs, 9, npts]\n",
    "    choose      : LongTensor [bs, 1, npts]\n",
    "    xmap, ymap: [bs, h, w]\n",
    "    K:          [bs, 3, 3]\n",
    "Returns:\n",
    "    end_points:\n",
    "\"\"\"\n",
    "'''\n",
    "item_dict = dict(\n",
    "    rgb=rgb.astype(np.uint8),  # [c, h, w]\n",
    "    cld_rgb_nrm=cld_rgb_nrm.astype(np.float32),  # [9, npts]\n",
    "    choose=choose.astype(np.int32),  # [1, npts]\n",
    "    labels=labels_pt.astype(np.int32),  # [npts]\n",
    "    rgb_labels=rgb_labels.astype(np.int32),  # [h, w]\n",
    "    dpt_map_m=dpt_m.astype(np.float32),  # [h, w]\n",
    "    RTs=RTs.astype(np.float32),\n",
    "    kp_targ_ofst=kp_targ_ofst.astype(np.float32),\n",
    "    ctr_targ_ofst=ctr_targ_ofst.astype(np.float32),\n",
    "    cls_ids=cls_ids.astype(np.int32),\n",
    "    ctr_3ds=ctr3ds.astype(np.float32),\n",
    "    kp_3ds=kp3ds.astype(np.float32),\n",
    ")\n",
    "'''\n",
    "\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "# Example image to tensor:\n",
    "example_rgb = Image.open(\"data/rgb/0010.png\")\n",
    "rgb_tensor = convert_tensor(example_rgb)\n",
    "# for a batch size of 2, stack tensors:\n",
    "rgb_stack = torch.stack((rgb_tensor, rgb_tensor), dim=0)\n",
    "print(rgb_stack.shape)\n",
    "print(rgb_stack.dtype)\n",
    "\n",
    "# 6Dim tensor for dpt + normal map\n",
    "dpt_tensor = torch.rand(6, 1920, 1440, device=\"cuda\") \n",
    "dpt_stack = torch.stack((dpt_tensor, dpt_tensor), dim=0)\n",
    "print(dpt_stack.shape)\n",
    "print(dpt_stack.dtype)\n",
    "\n",
    "example_cld_rgb_nrm = torch.rand((9, 8), device=\"cuda\")\n",
    "cld_rgb_nrm_stack = torch.stack((example_cld_rgb_nrm, example_cld_rgb_nrm), dim=0)\n",
    "print(cld_rgb_nrm_stack.shape)\n",
    "print(cld_rgb_nrm_stack.dtype)\n",
    "\n",
    "# choose npts to be 8\n",
    "example_choose = torch.randint(0, 1000, size=(1,8), device=\"cuda\")\n",
    "choose_long_tensor = torch.stack((example_choose,example_choose),dim=0)\n",
    "print(choose_long_tensor.shape)\n",
    "print(choose_long_tensor.dtype)\n",
    "\n",
    "#actual input size of images\n",
    "example_xy_map = torch.rand(1920, 1440, device=\"cuda\", dtype=torch.int)\n",
    "xy_map_stack = torch.stack((example_xy_map, example_xy_map), dim=0)\n",
    "print(xy_map_stack.shape)\n",
    "print(xy_map_stack.dtype)\n",
    "\n",
    "# use actual intrinsics matrix: it's already known & actually used in training\n",
    "example_intrinsics = torch.tensor(np.array([[1594.7247314453125, 0., 951.2391967773438],\n",
    "                                             [0., 1594.7247314453125, 722.7899761199951],\n",
    "                                             [0., 0., 1.]]), device=\"cuda\")\n",
    "intrinsics_stack = torch.stack((example_intrinsics, example_intrinsics), dim=0)\n",
    "print(intrinsics_stack.shape)\n",
    "print(intrinsics_stack.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c66890",
   "metadata": {},
   "source": [
    "As you can see, the random input tensors now correnspond to the types and shapes specified in the forward() method. Let's put them all into one input dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ba41d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rgb': tensor([[[[0.8745, 0.8706, 0.8627,  ..., 0.8314, 0.8235, 0.8078],\n",
      "          [0.8706, 0.8745, 0.8627,  ..., 0.8275, 0.8275, 0.8275],\n",
      "          [0.8706, 0.8667, 0.8588,  ..., 0.8275, 0.8314, 0.8314],\n",
      "          ...,\n",
      "          [0.8353, 0.8392, 0.8353,  ..., 0.8431, 0.8471, 0.8353],\n",
      "          [0.8157, 0.8471, 0.8392,  ..., 0.8392, 0.8471, 0.8235],\n",
      "          [0.8196, 0.8118, 0.8235,  ..., 0.8275, 0.8275, 0.7922]],\n",
      "\n",
      "         [[0.8627, 0.8667, 0.8627,  ..., 0.8157, 0.8118, 0.8196],\n",
      "          [0.8588, 0.8627, 0.8588,  ..., 0.8157, 0.8078, 0.8196],\n",
      "          [0.8627, 0.8627, 0.8549,  ..., 0.8235, 0.8196, 0.8275],\n",
      "          ...,\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.8353, 0.8353, 0.8196],\n",
      "          [0.8314, 0.8353, 0.8275,  ..., 0.8314, 0.8275, 0.8235],\n",
      "          [0.8196, 0.8275, 0.8235,  ..., 0.8275, 0.8235, 0.8157]],\n",
      "\n",
      "         [[0.8824, 0.8980, 0.8863,  ..., 0.8471, 0.8510, 0.8431],\n",
      "          [0.8902, 0.8824, 0.8824,  ..., 0.8510, 0.8471, 0.8510],\n",
      "          [0.8824, 0.8824, 0.8824,  ..., 0.8549, 0.8549, 0.8431],\n",
      "          ...,\n",
      "          [0.8667, 0.8627, 0.8588,  ..., 0.8627, 0.8627, 0.8392],\n",
      "          [0.8745, 0.8588, 0.8588,  ..., 0.8627, 0.8588, 0.8510],\n",
      "          [0.8392, 0.8471, 0.8588,  ..., 0.8471, 0.8510, 0.8431]]],\n",
      "\n",
      "\n",
      "        [[[0.8745, 0.8706, 0.8627,  ..., 0.8314, 0.8235, 0.8078],\n",
      "          [0.8706, 0.8745, 0.8627,  ..., 0.8275, 0.8275, 0.8275],\n",
      "          [0.8706, 0.8667, 0.8588,  ..., 0.8275, 0.8314, 0.8314],\n",
      "          ...,\n",
      "          [0.8353, 0.8392, 0.8353,  ..., 0.8431, 0.8471, 0.8353],\n",
      "          [0.8157, 0.8471, 0.8392,  ..., 0.8392, 0.8471, 0.8235],\n",
      "          [0.8196, 0.8118, 0.8235,  ..., 0.8275, 0.8275, 0.7922]],\n",
      "\n",
      "         [[0.8627, 0.8667, 0.8627,  ..., 0.8157, 0.8118, 0.8196],\n",
      "          [0.8588, 0.8627, 0.8588,  ..., 0.8157, 0.8078, 0.8196],\n",
      "          [0.8627, 0.8627, 0.8549,  ..., 0.8235, 0.8196, 0.8275],\n",
      "          ...,\n",
      "          [0.8275, 0.8314, 0.8314,  ..., 0.8353, 0.8353, 0.8196],\n",
      "          [0.8314, 0.8353, 0.8275,  ..., 0.8314, 0.8275, 0.8235],\n",
      "          [0.8196, 0.8275, 0.8235,  ..., 0.8275, 0.8235, 0.8157]],\n",
      "\n",
      "         [[0.8824, 0.8980, 0.8863,  ..., 0.8471, 0.8510, 0.8431],\n",
      "          [0.8902, 0.8824, 0.8824,  ..., 0.8510, 0.8471, 0.8510],\n",
      "          [0.8824, 0.8824, 0.8824,  ..., 0.8549, 0.8549, 0.8431],\n",
      "          ...,\n",
      "          [0.8667, 0.8627, 0.8588,  ..., 0.8627, 0.8627, 0.8392],\n",
      "          [0.8745, 0.8588, 0.8588,  ..., 0.8627, 0.8588, 0.8510],\n",
      "          [0.8392, 0.8471, 0.8588,  ..., 0.8471, 0.8510, 0.8431]]]]), 'dpt_nrm': tensor([[[[9.4474e-01, 1.2438e-01, 4.9885e-01,  ..., 7.7955e-02,\n",
      "           9.8598e-02, 8.9453e-01],\n",
      "          [6.0317e-01, 7.6068e-01, 1.0636e-01,  ..., 4.0782e-01,\n",
      "           8.2038e-01, 1.1039e-02],\n",
      "          [1.8914e-01, 3.7340e-01, 3.2543e-01,  ..., 4.7327e-01,\n",
      "           2.6688e-01, 7.9742e-01],\n",
      "          ...,\n",
      "          [5.7227e-01, 7.7022e-01, 1.0281e-01,  ..., 1.0528e-01,\n",
      "           6.3020e-01, 5.4321e-01],\n",
      "          [9.2234e-01, 2.7583e-01, 1.0226e-01,  ..., 5.8183e-01,\n",
      "           6.7073e-01, 9.0545e-01],\n",
      "          [4.7694e-01, 7.1636e-02, 2.2313e-01,  ..., 1.8362e-01,\n",
      "           9.2493e-01, 5.3872e-01]],\n",
      "\n",
      "         [[9.8986e-01, 3.7698e-01, 3.3581e-01,  ..., 5.2731e-02,\n",
      "           4.0275e-01, 8.9139e-01],\n",
      "          [6.5148e-01, 3.9509e-01, 8.2916e-01,  ..., 8.7490e-01,\n",
      "           5.8275e-01, 3.8878e-01],\n",
      "          [7.1137e-01, 3.3427e-01, 4.8165e-01,  ..., 9.9766e-01,\n",
      "           9.9560e-01, 6.6042e-01],\n",
      "          ...,\n",
      "          [9.6119e-01, 2.6840e-01, 8.0855e-01,  ..., 7.3406e-01,\n",
      "           7.4142e-01, 9.7697e-01],\n",
      "          [6.1350e-01, 5.6004e-01, 3.9843e-01,  ..., 1.8900e-01,\n",
      "           8.6879e-01, 4.1520e-01],\n",
      "          [3.0929e-01, 5.8388e-01, 3.4605e-01,  ..., 2.7523e-01,\n",
      "           1.8006e-01, 9.8014e-01]],\n",
      "\n",
      "         [[9.5285e-01, 3.3282e-01, 1.6282e-01,  ..., 7.7390e-01,\n",
      "           4.8796e-01, 5.2758e-01],\n",
      "          [3.4160e-01, 6.5850e-01, 5.3666e-01,  ..., 8.4012e-01,\n",
      "           8.9855e-01, 8.7781e-01],\n",
      "          [3.5392e-01, 7.0546e-01, 4.1993e-01,  ..., 9.4243e-01,\n",
      "           6.1791e-01, 2.3036e-01],\n",
      "          ...,\n",
      "          [4.7490e-01, 6.5437e-01, 2.8604e-01,  ..., 6.0438e-01,\n",
      "           6.7645e-01, 7.9436e-01],\n",
      "          [3.9121e-01, 5.0639e-01, 2.8732e-01,  ..., 7.6930e-01,\n",
      "           2.1211e-01, 4.7148e-01],\n",
      "          [8.5495e-01, 3.3424e-01, 6.3216e-01,  ..., 1.2675e-01,\n",
      "           3.1575e-01, 6.2603e-01]],\n",
      "\n",
      "         [[7.9620e-01, 3.5272e-02, 9.2919e-01,  ..., 1.6612e-01,\n",
      "           4.3352e-01, 7.4974e-01],\n",
      "          [2.5949e-01, 6.5764e-01, 7.3048e-01,  ..., 8.0433e-01,\n",
      "           1.6439e-01, 1.9822e-02],\n",
      "          [5.6820e-03, 9.5864e-01, 3.4935e-01,  ..., 1.3609e-01,\n",
      "           1.4559e-01, 1.5510e-01],\n",
      "          ...,\n",
      "          [9.7474e-01, 2.0839e-01, 2.7195e-01,  ..., 8.7270e-01,\n",
      "           8.9002e-01, 1.3793e-01],\n",
      "          [4.7755e-01, 3.5049e-01, 3.4171e-01,  ..., 7.8730e-01,\n",
      "           7.1833e-01, 9.1321e-01],\n",
      "          [5.6071e-01, 6.9098e-01, 3.3460e-01,  ..., 6.5051e-01,\n",
      "           9.8209e-01, 3.2903e-01]],\n",
      "\n",
      "         [[1.6612e-01, 2.3981e-01, 8.5225e-01,  ..., 4.3929e-01,\n",
      "           5.0623e-01, 3.1945e-02],\n",
      "          [3.7789e-01, 5.0018e-01, 4.0261e-01,  ..., 3.3944e-01,\n",
      "           9.9077e-01, 2.5933e-01],\n",
      "          [8.0519e-01, 9.2783e-02, 9.6379e-01,  ..., 2.1874e-01,\n",
      "           4.7955e-04, 9.8570e-01],\n",
      "          ...,\n",
      "          [3.0735e-01, 3.9817e-01, 8.6181e-01,  ..., 1.0184e-01,\n",
      "           5.1185e-01, 8.4855e-01],\n",
      "          [7.3971e-02, 1.0248e-02, 2.8753e-01,  ..., 7.6112e-01,\n",
      "           3.3588e-01, 1.0904e-01],\n",
      "          [7.9974e-01, 3.5760e-01, 1.3050e-01,  ..., 4.9689e-02,\n",
      "           5.5029e-01, 5.3855e-01]],\n",
      "\n",
      "         [[5.6375e-01, 9.1965e-01, 4.7430e-01,  ..., 4.7197e-01,\n",
      "           1.5042e-02, 7.4135e-01],\n",
      "          [4.7888e-01, 6.3926e-01, 6.1368e-01,  ..., 9.0685e-01,\n",
      "           1.0403e-01, 6.2729e-01],\n",
      "          [2.7512e-01, 5.7740e-01, 3.0066e-01,  ..., 8.5057e-01,\n",
      "           3.3597e-01, 1.4900e-01],\n",
      "          ...,\n",
      "          [6.8749e-01, 5.7458e-01, 9.4586e-01,  ..., 8.5530e-01,\n",
      "           5.1476e-01, 6.7807e-01],\n",
      "          [7.5532e-01, 7.0853e-02, 4.4498e-01,  ..., 7.5191e-01,\n",
      "           4.9519e-01, 1.4063e-01],\n",
      "          [5.5053e-01, 8.3227e-01, 4.4777e-01,  ..., 2.5079e-01,\n",
      "           5.1824e-01, 7.0087e-01]]],\n",
      "\n",
      "\n",
      "        [[[9.4474e-01, 1.2438e-01, 4.9885e-01,  ..., 7.7955e-02,\n",
      "           9.8598e-02, 8.9453e-01],\n",
      "          [6.0317e-01, 7.6068e-01, 1.0636e-01,  ..., 4.0782e-01,\n",
      "           8.2038e-01, 1.1039e-02],\n",
      "          [1.8914e-01, 3.7340e-01, 3.2543e-01,  ..., 4.7327e-01,\n",
      "           2.6688e-01, 7.9742e-01],\n",
      "          ...,\n",
      "          [5.7227e-01, 7.7022e-01, 1.0281e-01,  ..., 1.0528e-01,\n",
      "           6.3020e-01, 5.4321e-01],\n",
      "          [9.2234e-01, 2.7583e-01, 1.0226e-01,  ..., 5.8183e-01,\n",
      "           6.7073e-01, 9.0545e-01],\n",
      "          [4.7694e-01, 7.1636e-02, 2.2313e-01,  ..., 1.8362e-01,\n",
      "           9.2493e-01, 5.3872e-01]],\n",
      "\n",
      "         [[9.8986e-01, 3.7698e-01, 3.3581e-01,  ..., 5.2731e-02,\n",
      "           4.0275e-01, 8.9139e-01],\n",
      "          [6.5148e-01, 3.9509e-01, 8.2916e-01,  ..., 8.7490e-01,\n",
      "           5.8275e-01, 3.8878e-01],\n",
      "          [7.1137e-01, 3.3427e-01, 4.8165e-01,  ..., 9.9766e-01,\n",
      "           9.9560e-01, 6.6042e-01],\n",
      "          ...,\n",
      "          [9.6119e-01, 2.6840e-01, 8.0855e-01,  ..., 7.3406e-01,\n",
      "           7.4142e-01, 9.7697e-01],\n",
      "          [6.1350e-01, 5.6004e-01, 3.9843e-01,  ..., 1.8900e-01,\n",
      "           8.6879e-01, 4.1520e-01],\n",
      "          [3.0929e-01, 5.8388e-01, 3.4605e-01,  ..., 2.7523e-01,\n",
      "           1.8006e-01, 9.8014e-01]],\n",
      "\n",
      "         [[9.5285e-01, 3.3282e-01, 1.6282e-01,  ..., 7.7390e-01,\n",
      "           4.8796e-01, 5.2758e-01],\n",
      "          [3.4160e-01, 6.5850e-01, 5.3666e-01,  ..., 8.4012e-01,\n",
      "           8.9855e-01, 8.7781e-01],\n",
      "          [3.5392e-01, 7.0546e-01, 4.1993e-01,  ..., 9.4243e-01,\n",
      "           6.1791e-01, 2.3036e-01],\n",
      "          ...,\n",
      "          [4.7490e-01, 6.5437e-01, 2.8604e-01,  ..., 6.0438e-01,\n",
      "           6.7645e-01, 7.9436e-01],\n",
      "          [3.9121e-01, 5.0639e-01, 2.8732e-01,  ..., 7.6930e-01,\n",
      "           2.1211e-01, 4.7148e-01],\n",
      "          [8.5495e-01, 3.3424e-01, 6.3216e-01,  ..., 1.2675e-01,\n",
      "           3.1575e-01, 6.2603e-01]],\n",
      "\n",
      "         [[7.9620e-01, 3.5272e-02, 9.2919e-01,  ..., 1.6612e-01,\n",
      "           4.3352e-01, 7.4974e-01],\n",
      "          [2.5949e-01, 6.5764e-01, 7.3048e-01,  ..., 8.0433e-01,\n",
      "           1.6439e-01, 1.9822e-02],\n",
      "          [5.6820e-03, 9.5864e-01, 3.4935e-01,  ..., 1.3609e-01,\n",
      "           1.4559e-01, 1.5510e-01],\n",
      "          ...,\n",
      "          [9.7474e-01, 2.0839e-01, 2.7195e-01,  ..., 8.7270e-01,\n",
      "           8.9002e-01, 1.3793e-01],\n",
      "          [4.7755e-01, 3.5049e-01, 3.4171e-01,  ..., 7.8730e-01,\n",
      "           7.1833e-01, 9.1321e-01],\n",
      "          [5.6071e-01, 6.9098e-01, 3.3460e-01,  ..., 6.5051e-01,\n",
      "           9.8209e-01, 3.2903e-01]],\n",
      "\n",
      "         [[1.6612e-01, 2.3981e-01, 8.5225e-01,  ..., 4.3929e-01,\n",
      "           5.0623e-01, 3.1945e-02],\n",
      "          [3.7789e-01, 5.0018e-01, 4.0261e-01,  ..., 3.3944e-01,\n",
      "           9.9077e-01, 2.5933e-01],\n",
      "          [8.0519e-01, 9.2783e-02, 9.6379e-01,  ..., 2.1874e-01,\n",
      "           4.7955e-04, 9.8570e-01],\n",
      "          ...,\n",
      "          [3.0735e-01, 3.9817e-01, 8.6181e-01,  ..., 1.0184e-01,\n",
      "           5.1185e-01, 8.4855e-01],\n",
      "          [7.3971e-02, 1.0248e-02, 2.8753e-01,  ..., 7.6112e-01,\n",
      "           3.3588e-01, 1.0904e-01],\n",
      "          [7.9974e-01, 3.5760e-01, 1.3050e-01,  ..., 4.9689e-02,\n",
      "           5.5029e-01, 5.3855e-01]],\n",
      "\n",
      "         [[5.6375e-01, 9.1965e-01, 4.7430e-01,  ..., 4.7197e-01,\n",
      "           1.5042e-02, 7.4135e-01],\n",
      "          [4.7888e-01, 6.3926e-01, 6.1368e-01,  ..., 9.0685e-01,\n",
      "           1.0403e-01, 6.2729e-01],\n",
      "          [2.7512e-01, 5.7740e-01, 3.0066e-01,  ..., 8.5057e-01,\n",
      "           3.3597e-01, 1.4900e-01],\n",
      "          ...,\n",
      "          [6.8749e-01, 5.7458e-01, 9.4586e-01,  ..., 8.5530e-01,\n",
      "           5.1476e-01, 6.7807e-01],\n",
      "          [7.5532e-01, 7.0853e-02, 4.4498e-01,  ..., 7.5191e-01,\n",
      "           4.9519e-01, 1.4063e-01],\n",
      "          [5.5053e-01, 8.3227e-01, 4.4777e-01,  ..., 2.5079e-01,\n",
      "           5.1824e-01, 7.0087e-01]]]], device='cuda:0'), 'cld_rgb_nrm': tensor([[[0.8019, 0.0749, 0.3561, 0.7163, 0.5352, 0.4099, 0.7767, 0.3595],\n",
      "         [0.3555, 0.0810, 0.3385, 0.5673, 0.8387, 0.0133, 0.2970, 0.5121],\n",
      "         [0.0182, 0.8504, 0.2250, 0.1066, 0.7059, 0.4385, 0.0074, 0.5307],\n",
      "         [0.2301, 0.5253, 0.2394, 0.5102, 0.0283, 0.3125, 0.7762, 0.8196],\n",
      "         [0.0657, 0.9547, 0.7591, 0.2208, 0.8965, 0.4866, 0.3039, 0.7740],\n",
      "         [0.8987, 0.6000, 0.6987, 0.4976, 0.1048, 0.1245, 0.8296, 0.8865],\n",
      "         [0.5314, 0.1639, 0.1914, 0.4879, 0.6058, 0.0533, 0.3990, 0.3641],\n",
      "         [0.4346, 0.6527, 0.5097, 0.4438, 0.6397, 0.7179, 0.2470, 0.7000],\n",
      "         [0.8329, 0.0104, 0.5045, 0.7911, 0.0776, 0.0986, 0.7957, 0.9196]],\n",
      "\n",
      "        [[0.8019, 0.0749, 0.3561, 0.7163, 0.5352, 0.4099, 0.7767, 0.3595],\n",
      "         [0.3555, 0.0810, 0.3385, 0.5673, 0.8387, 0.0133, 0.2970, 0.5121],\n",
      "         [0.0182, 0.8504, 0.2250, 0.1066, 0.7059, 0.4385, 0.0074, 0.5307],\n",
      "         [0.2301, 0.5253, 0.2394, 0.5102, 0.0283, 0.3125, 0.7762, 0.8196],\n",
      "         [0.0657, 0.9547, 0.7591, 0.2208, 0.8965, 0.4866, 0.3039, 0.7740],\n",
      "         [0.8987, 0.6000, 0.6987, 0.4976, 0.1048, 0.1245, 0.8296, 0.8865],\n",
      "         [0.5314, 0.1639, 0.1914, 0.4879, 0.6058, 0.0533, 0.3990, 0.3641],\n",
      "         [0.4346, 0.6527, 0.5097, 0.4438, 0.6397, 0.7179, 0.2470, 0.7000],\n",
      "         [0.8329, 0.0104, 0.5045, 0.7911, 0.0776, 0.0986, 0.7957, 0.9196]]],\n",
      "       device='cuda:0'), 'choose': tensor([[[849,  83, 788, 867, 451, 462, 101, 650]],\n",
      "\n",
      "        [[849,  83, 788, 867, 451, 462, 101, 650]]], device='cuda:0'), 'xmap': tensor([[[0.5386, 0.3773, 0.0249,  ..., 0.1724, 0.1920, 0.2330],\n",
      "         [0.6459, 0.2872, 0.1547,  ..., 0.1082, 0.4754, 0.9130],\n",
      "         [0.0972, 0.3488, 0.9582,  ..., 0.6666, 0.3353, 0.1526],\n",
      "         ...,\n",
      "         [0.5389, 0.1127, 0.0967,  ..., 0.3654, 0.9669, 0.4115],\n",
      "         [0.7507, 0.9113, 0.3944,  ..., 0.4597, 0.1302, 0.7978],\n",
      "         [0.0975, 0.5745, 0.7927,  ..., 0.6971, 0.5750, 0.7271]],\n",
      "\n",
      "        [[0.5386, 0.3773, 0.0249,  ..., 0.1724, 0.1920, 0.2330],\n",
      "         [0.6459, 0.2872, 0.1547,  ..., 0.1082, 0.4754, 0.9130],\n",
      "         [0.0972, 0.3488, 0.9582,  ..., 0.6666, 0.3353, 0.1526],\n",
      "         ...,\n",
      "         [0.5389, 0.1127, 0.0967,  ..., 0.3654, 0.9669, 0.4115],\n",
      "         [0.7507, 0.9113, 0.3944,  ..., 0.4597, 0.1302, 0.7978],\n",
      "         [0.0975, 0.5745, 0.7927,  ..., 0.6971, 0.5750, 0.7271]]],\n",
      "       device='cuda:0'), 'ymap': tensor([[[0.5386, 0.3773, 0.0249,  ..., 0.1724, 0.1920, 0.2330],\n",
      "         [0.6459, 0.2872, 0.1547,  ..., 0.1082, 0.4754, 0.9130],\n",
      "         [0.0972, 0.3488, 0.9582,  ..., 0.6666, 0.3353, 0.1526],\n",
      "         ...,\n",
      "         [0.5389, 0.1127, 0.0967,  ..., 0.3654, 0.9669, 0.4115],\n",
      "         [0.7507, 0.9113, 0.3944,  ..., 0.4597, 0.1302, 0.7978],\n",
      "         [0.0975, 0.5745, 0.7927,  ..., 0.6971, 0.5750, 0.7271]],\n",
      "\n",
      "        [[0.5386, 0.3773, 0.0249,  ..., 0.1724, 0.1920, 0.2330],\n",
      "         [0.6459, 0.2872, 0.1547,  ..., 0.1082, 0.4754, 0.9130],\n",
      "         [0.0972, 0.3488, 0.9582,  ..., 0.6666, 0.3353, 0.1526],\n",
      "         ...,\n",
      "         [0.5389, 0.1127, 0.0967,  ..., 0.3654, 0.9669, 0.4115],\n",
      "         [0.7507, 0.9113, 0.3944,  ..., 0.4597, 0.1302, 0.7978],\n",
      "         [0.0975, 0.5745, 0.7927,  ..., 0.6971, 0.5750, 0.7271]]],\n",
      "       device='cuda:0'), 'K': tensor([[[1.5947e+03, 0.0000e+00, 9.5124e+02],\n",
      "         [0.0000e+00, 1.5947e+03, 7.2279e+02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
      "\n",
      "        [[1.5947e+03, 0.0000e+00, 9.5124e+02],\n",
      "         [0.0000e+00, 1.5947e+03, 7.2279e+02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]], device='cuda:0',\n",
      "       dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "inputs[\"rgb\"] = rgb_stack\n",
    "inputs[\"dpt_nrm\"] = dpt_stack\n",
    "inputs[\"cld_rgb_nrm\"] = cld_rgb_nrm_stack\n",
    "inputs[\"choose\"] = choose_long_tensor\n",
    "inputs[\"xmap\"] = xy_map_stack\n",
    "inputs[\"ymap\"] = xy_map_stack\n",
    "inputs[\"K\"] = intrinsics_stack\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13b7d1e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-3c89ae4a5bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for key, value in inputs.items():\n",
    "        value.to('cuda')\n",
    "    model = model.to('cuda')\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ed7aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../ffb6d/train_log/custom/checkpoints/vase/FFB6D_vase_best.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0c67bd9a99a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_tar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel_tar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../extracted_model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# specify which folder to extract to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_tar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ffb6d/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m                     \u001b[0msaved_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ffb6d/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ffb6d/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../ffb6d/train_log/custom/checkpoints/vase/FFB6D_vase_best.pth.tar'"
     ]
    }
   ],
   "source": [
    "# An instance of your model.\n",
    "cls = \"vase\"\n",
    "model_str = \"../ffb6d/train_log/custom/checkpoints/{0}/FFB6D_{0}_best.pth\".format(cls)\n",
    "model_tar = model_str + \".tar\"\n",
    "\n",
    "model_tar = tarfile.open(model_tar)\n",
    "model_tar.extractall('../extracted_model') # specify which folder to extract to\n",
    "model_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load(model_str)\n",
    "\n",
    "# Switch the model to eval model\n",
    "model.eval()\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_module = torch.jit.script(model, example)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_module.save(\"traced_ffb6d_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
