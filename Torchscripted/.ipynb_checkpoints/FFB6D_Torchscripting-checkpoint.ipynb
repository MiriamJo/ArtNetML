{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248291c5",
   "metadata": {},
   "source": [
    "# Convert FFB6D to TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db67a3",
   "metadata": {},
   "source": [
    "After cloning this repositiory please follow the instructions on compiling apex, normalspeed and RandLA.\n",
    "After you obtained a FFB6D model, you can follow these steps to convert it into Torchschript:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23846",
   "metadata": {},
   "source": [
    "# Add configuration for RandLAN & PSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407c0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b3b55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if Cuda is installed properly\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea821958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Sun_Mar_21_19:15:46_PDT_2021\r\n",
      "Cuda compilation tools, release 11.3, V11.3.58\r\n",
      "Build cuda_11.3.r11.3/compiler.29745058_0\r\n"
     ]
    }
   ],
   "source": [
    "#If \"True\": ignore this and next command\n",
    "#If \"False\": Check if cuda is installed properly with\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a387963",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-b9e7614b51c1>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-b9e7614b51c1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# If not: Install cuda toolkit & CudNN driver\n",
    "# If version shows: run this command with an active conda environment:\n",
    "cuda_version = 11.3 # your installed cuda version\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d8846",
   "metadata": {},
   "source": [
    "## Add PSPNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super(PSPModule, self).__init__()\n",
    "        self.stages = []\n",
    "        self.stages = nn.ModuleList(\n",
    "            [self._make_stage(features, size) for size in sizes]\n",
    "        )\n",
    "        self.bottleneck = nn.Conv2d(\n",
    "            features * (len(sizes) + 1), out_features, kernel_size=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)\n",
    "        priors = [\n",
    "            F.upsample(input=stage(feats), size=(h, w), mode='bilinear')\n",
    "            for stage in self.stages\n",
    "        ] + [feats]\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)\n",
    "\n",
    "\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PSPUpsample, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Modified_PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "                 deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "                 ):\n",
    "        super(Modified_PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "            deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "    ):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            # nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da0631",
   "metadata": {},
   "source": [
    "## Add RandLANet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55618736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.fc0 = pt_utils.Conv1d(config.in_c, 8, kernel_size=1, bn=True)\n",
    "\n",
    "        self.dilated_res_blocks = nn.ModuleList()\n",
    "        d_in = 8\n",
    "        for i in range(self.config.num_layers):\n",
    "            d_out = self.config.d_out[i]\n",
    "            self.dilated_res_blocks.append(Dilated_res_block(d_in, d_out))\n",
    "            d_in = 2 * d_out\n",
    "\n",
    "        d_out = d_in\n",
    "        self.decoder_0 = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        for j in range(self.config.num_layers):\n",
    "            if j < 3:\n",
    "                d_in = d_out + 2 * self.config.d_out[-j-2]\n",
    "                d_out = 2 * self.config.d_out[-j-2]\n",
    "            else:\n",
    "                d_in = 4 * self.config.d_out[-4]\n",
    "                d_out = 2 * self.config.d_out[-4]\n",
    "            self.decoder_blocks.append(pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True))\n",
    "\n",
    "        self.fc1 = pt_utils.Conv2d(d_out, 64, kernel_size=(1,1), bn=True)\n",
    "        self.fc2 = pt_utils.Conv2d(64, 32, kernel_size=(1,1), bn=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = pt_utils.Conv2d(32, self.config.num_classes, kernel_size=(1,1), bn=False, activation=None)\n",
    "\n",
    "    def forward(self, end_points):\n",
    "\n",
    "        features = end_points['features']  # Batch*channel*npoints\n",
    "        features = self.fc0(features)\n",
    "\n",
    "        features = features.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###########################Encoder############################\n",
    "        f_encoder_list = []\n",
    "        for i in range(self.config.num_layers):\n",
    "            f_encoder_i = self.dilated_res_blocks[i](\n",
    "                features, end_points['xyz'][i], end_points['neigh_idx'][i]\n",
    "            )\n",
    "\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, end_points['sub_idx'][i])\n",
    "            features = f_sampled_i\n",
    "            print(\"encoder%d:\"%i, features.size())\n",
    "            if i == 0:\n",
    "                f_encoder_list.append(f_encoder_i)\n",
    "            f_encoder_list.append(f_sampled_i)\n",
    "        # ###########################Encoder############################\n",
    "\n",
    "        features = self.decoder_0(f_encoder_list[-1])\n",
    "\n",
    "        # ###########################Decoder############################\n",
    "        f_decoder_list = []\n",
    "        for j in range(self.config.num_layers):\n",
    "            f_interp_i = self.nearest_interpolation(features, end_points['interp_idx'][-j - 1])\n",
    "            f_decoder_i = self.decoder_blocks[j](torch.cat([f_encoder_list[-j - 2], f_interp_i], dim=1))\n",
    "\n",
    "            features = f_decoder_i\n",
    "            print(\"decoder%d:\"%j, features.size())\n",
    "            f_decoder_list.append(f_decoder_i)\n",
    "        # ###########################Decoder############################\n",
    "\n",
    "        features = self.fc1(features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc3(features)\n",
    "        f_out = features.squeeze(3)\n",
    "\n",
    "        end_points['logits'] = f_out\n",
    "        return end_points\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1))\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, C, npoints] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, c, up_num_points, 1] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(feature, 2, interp_idx.unsqueeze(1).repeat(1,feature.shape[1],1))\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "\n",
    "\n",
    "def compute_acc(end_points):\n",
    "\n",
    "    logits = end_points['valid_logits']\n",
    "    labels = end_points['valid_labels']\n",
    "    logits = logits.max(dim=1)[1]\n",
    "    acc = (logits == labels).sum().float() / float(labels.shape[0])\n",
    "    end_points['acc'] = acc\n",
    "    return acc, end_points\n",
    "\n",
    "\n",
    "class IoUCalculator:\n",
    "    def __init__(self, cfg):\n",
    "        self.gt_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.true_positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def add_data(self, end_points):\n",
    "        logits = end_points['valid_logits']\n",
    "        labels = end_points['valid_labels']\n",
    "        pred = logits.max(dim=1)[1]\n",
    "        pred_valid = pred.detach().cpu().numpy()\n",
    "        labels_valid = labels.detach().cpu().numpy()\n",
    "\n",
    "        val_total_correct = 0\n",
    "        val_total_seen = 0\n",
    "\n",
    "        correct = np.sum(pred_valid == labels_valid)\n",
    "        val_total_correct += correct\n",
    "        val_total_seen += len(labels_valid)\n",
    "\n",
    "        conf_matrix = confusion_matrix(labels_valid, pred_valid, np.arange(0, self.cfg.num_classes, 1))\n",
    "        self.gt_classes += np.sum(conf_matrix, axis=1)\n",
    "        self.positive_classes += np.sum(conf_matrix, axis=0)\n",
    "        self.true_positive_classes += np.diagonal(conf_matrix)\n",
    "\n",
    "    def compute_iou(self):\n",
    "        iou_list = []\n",
    "        for n in range(0, self.cfg.num_classes, 1):\n",
    "            if float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n]) != 0:\n",
    "                iou = self.true_positive_classes[n] / float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n])\n",
    "                iou_list.append(iou)\n",
    "            else:\n",
    "                iou_list.append(0.0)\n",
    "        mean_iou = sum(iou_list) / float(self.cfg.num_classes)\n",
    "        return mean_iou, iou_list\n",
    "\n",
    "\n",
    "\n",
    "class Dilated_res_block(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = pt_utils.Conv2d(d_in, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.lfa = Building_block(d_out)\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out, d_out*2, kernel_size=(1, 1), bn=True, activation=None)\n",
    "        self.shortcut = pt_utils.Conv2d(d_in, d_out*2, kernel_size=(1,1), bn=True, activation=None)\n",
    "\n",
    "    def forward(self, feature, xyz, neigh_idx):\n",
    "        f_pc = self.mlp1(feature)  # Batch*channel*npoints*1\n",
    "        f_pc = self.lfa(xyz, f_pc, neigh_idx)  # Batch*d_out*npoints*1\n",
    "        f_pc = self.mlp2(f_pc)\n",
    "        shortcut = self.shortcut(feature)\n",
    "        return F.leaky_relu(f_pc+shortcut, negative_slope=0.2)\n",
    "\n",
    "\n",
    "class Building_block(nn.Module):\n",
    "    def __init__(self, d_out):  #  d_in = d_out//2\n",
    "        super().__init__()\n",
    "        self.mlp1 = pt_utils.Conv2d(10, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.att_pooling_1 = Att_pooling(d_out, d_out//2)\n",
    "\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out//2, d_out//2, kernel_size=(1, 1), bn=True)\n",
    "        self.att_pooling_2 = Att_pooling(d_out, d_out)\n",
    "\n",
    "    def forward(self, xyz, feature, neigh_idx):  # feature: Batch*channel*npoints*1\n",
    "        f_xyz = self.relative_pos_encoding(xyz, neigh_idx)  # batch*npoint*nsamples*10\n",
    "        f_xyz = f_xyz.permute((0, 3, 1, 2)).contiguous()  # batch*10*npoint*nsamples\n",
    "        f_xyz = self.mlp1(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            feature.squeeze(-1).permute((0, 2, 1)).contiguous(),neigh_idx\n",
    "        )  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_1(f_concat)  # Batch*channel*npoints*1\n",
    "\n",
    "        f_xyz = self.mlp2(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            f_pc_agg.squeeze(-1).permute((0, 2, 1)).contiguous(), neigh_idx\n",
    "        ).contiguous()  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_2(f_concat)\n",
    "        return f_pc_agg\n",
    "\n",
    "    def relative_pos_encoding(self, xyz, neigh_idx):\n",
    "        neighbor_xyz = self.gather_neighbour(xyz, neigh_idx)  # batch*npoint*nsamples*3\n",
    "\n",
    "        xyz_tile = xyz.unsqueeze(2).repeat(1, 1, neigh_idx.shape[-1], 1)  # batch*npoint*nsamples*3\n",
    "        relative_xyz = xyz_tile - neighbor_xyz  # batch*npoint*nsamples*3\n",
    "        relative_dis = torch.sqrt(torch.sum(torch.pow(relative_xyz, 2), dim=-1, keepdim=True))  # batch*npoint*nsamples*1\n",
    "        relative_feature = torch.cat([relative_dis, relative_xyz, xyz_tile, neighbor_xyz], dim=-1)  # batch*npoint*nsamples*10\n",
    "        return relative_feature\n",
    "\n",
    "    @staticmethod\n",
    "    def gather_neighbour(pc, neighbor_idx):  # pc: batch*npoint*channel\n",
    "        # gather the coordinates or features of neighboring points\n",
    "        batch_size = pc.shape[0]\n",
    "        num_points = pc.shape[1]\n",
    "        d = pc.shape[2]\n",
    "        index_input = neighbor_idx.reshape(batch_size, -1)\n",
    "        features = torch.gather(pc, 1, index_input.unsqueeze(-1).repeat(1, 1, pc.shape[2])).contiguous()\n",
    "        features = features.reshape(batch_size, num_points, neighbor_idx.shape[-1], d)  # batch*npoint*nsamples*channel\n",
    "        return features\n",
    "\n",
    "\n",
    "class Att_pooling(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Conv2d(d_in, d_in, (1, 1), bias=False)\n",
    "        self.mlp = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "    def forward(self, feature_set):\n",
    "\n",
    "        att_activation = self.fc(feature_set)\n",
    "        att_scores = F.softmax(att_activation, dim=3)\n",
    "        f_agg = feature_set * att_scores\n",
    "        f_agg = torch.sum(f_agg, dim=3, keepdim=True)\n",
    "        f_agg = self.mlp(f_agg)\n",
    "        return f_agg\n",
    "\n",
    "\n",
    "def compute_loss(end_points, cfg):\n",
    "\n",
    "    logits = end_points['logits']\n",
    "    labels = end_points['labels']\n",
    "\n",
    "    logits = logits.transpose(1, 2).reshape(-1, cfg.num_classes)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    # Boolean mask of points that should be ignored\n",
    "    ignored_bool = labels == 0\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        ignored_bool = ignored_bool | (labels == ign_label)\n",
    "\n",
    "    # Collect logits and labels that are not ignored\n",
    "    valid_idx = ignored_bool == 0\n",
    "    valid_logits = logits[valid_idx, :]\n",
    "    valid_labels_init = labels[valid_idx]\n",
    "\n",
    "    # Reduce label values in the range of logit shape\n",
    "    reducing_list = torch.range(0, cfg.num_classes).long().cuda()\n",
    "    inserted_value = torch.zeros((1,)).long().cuda()\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        reducing_list = torch.cat([reducing_list[:ign_label], inserted_value, reducing_list[ign_label:]], 0)\n",
    "    valid_labels = torch.gather(reducing_list, 0, valid_labels_init)\n",
    "    loss = get_loss(valid_logits, valid_labels, cfg.class_weights)\n",
    "    end_points['valid_logits'], end_points['valid_labels'] = valid_logits, valid_labels\n",
    "    end_points['loss'] = loss\n",
    "    return loss, end_points\n",
    "\n",
    "\n",
    "def get_loss(logits, labels, pre_cal_weights):\n",
    "    # calculate the weighted cross entropy according to the inverse frequency\n",
    "    class_weights = torch.from_numpy(pre_cal_weights).float().cuda()\n",
    "    # one_hot_labels = F.one_hot(labels, self.config.num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='none')\n",
    "    output_loss = criterion(logits, labels)\n",
    "    output_loss = output_loss.mean()\n",
    "    return output_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53764e",
   "metadata": {},
   "source": [
    "# FFB6D\n",
    "\n",
    "Finally add FFB6D and trace the model flow with TorchJitScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "}\n",
    "\n",
    "\n",
    "class FFB6D(nn.Module):\n",
    "    def __init__(self, n_classes, n_pts, rndla_cfg, n_kps=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # ######################## prepare stages#########################\n",
    "        self.n_cls = n_classes\n",
    "        self.n_pts = n_pts\n",
    "        self.n_kps = n_kps\n",
    "        cnn = psp_models['resnet34'.lower()]()\n",
    "\n",
    "        rndla = RandLANet(rndla_cfg)\n",
    "\n",
    "        self.cnn_pre_stages = nn.Sequential (\n",
    "            cnn.feats.conv1,  # stride = 2, [bs, c, 240, 320]\n",
    "            cnn.feats.bn1, cnn.feats.relu,\n",
    "            cnn.feats.maxpool  # stride = 2, [bs, 64, 120, 160]\n",
    "        )\n",
    "        self.rndla_pre_stages = rndla.fc0\n",
    "\n",
    "        # ####################### downsample stages#######################\n",
    "        self.cnn_ds_stages = nn.ModuleList([\n",
    "            cnn.feats.layer1,    # stride = 1, [bs, 64, 120, 160]\n",
    "            cnn.feats.layer2,    # stride = 2, [bs, 128, 60, 80]\n",
    "            # stride = 1, [bs, 128, 60, 80]\n",
    "            nn.Sequential(cnn.feats.layer3, cnn.feats.layer4),\n",
    "            nn.Sequential(cnn.psp, cnn.drop_1)   # [bs, 1024, 60, 80]\n",
    "        ])\n",
    "        self.ds_sr = [4, 8, 8, 8]\n",
    "\n",
    "        self.rndla_ds_stages = rndla.dilated_res_blocks\n",
    "\n",
    "        self.ds_rgb_oc = [64, 128, 512, 1024]\n",
    "        self.ds_rndla_oc = [item * 2 for item in rndla_cfg.d_out]\n",
    "        self.ds_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.ds_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i], self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i]*2, self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.ds_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i], self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i]*2, self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ###################### upsample stages #############################\n",
    "        self.cnn_up_stages = nn.ModuleList([\n",
    "            nn.Sequential(cnn.up_1, cnn.drop_2),  # [bs, 256, 120, 160]\n",
    "            nn.Sequential(cnn.up_2, cnn.drop_2),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.final),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.up_3, cnn.final)  # [bs, 64, 480, 640]\n",
    "        ])\n",
    "        self.up_rgb_oc = [256, 64, 64]\n",
    "        self.up_rndla_oc = []\n",
    "        for j in range(rndla_cfg.num_layers):\n",
    "            if j < 3:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[-j-2])\n",
    "            else:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[0])\n",
    "\n",
    "        self.rndla_up_stages = rndla.decoder_blocks\n",
    "\n",
    "        n_fuse_layer = 3\n",
    "        self.up_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(n_fuse_layer):\n",
    "            self.up_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i], self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i]*2, self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.up_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i], self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i]*2, self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ####################### prediction headers #############################\n",
    "        # We use 3D keypoint prediction header for pose estimation following PVN3D\n",
    "        # You can use different prediction headers for different downstream tasks.\n",
    "\n",
    "        self.rgbd_seg_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1] + self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_classes, activation=None)\n",
    "        )\n",
    "\n",
    "        self.ctr_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(3, activation=None)\n",
    "        )\n",
    "\n",
    "        self.kp_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_kps*3, activation=None)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        if len(feature.size()) > 3:\n",
    "            feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(\n",
    "            feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, up_num_points, d] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(\n",
    "            feature, 2, interp_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "    def _break_up_pc(self, pc):\n",
    "        xyz = pc[:, :3, :].transpose(1, 2).contiguous()\n",
    "        features = (\n",
    "            pc[:, 3:, :].contiguous() if pc.size(1) > 3 else None\n",
    "        )\n",
    "        return xyz, features\n",
    "\n",
    "    def forward(\n",
    "        self, inputs, end_points=None, scale=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        inputs: dict of :\n",
    "            rgb         : FloatTensor [bs, 3, h, w]\n",
    "            dpt_nrm     : FloatTensor [bs, 6, h, w], 3c xyz in meter + 3c normal map\n",
    "            cld_rgb_nrm : FloatTensor [bs, 9, npts]\n",
    "            choose      : LongTensor [bs, 1, npts]\n",
    "            xmap, ymap: [bs, h, w]\n",
    "            K:          [bs, 3, 3]\n",
    "        Returns:\n",
    "            end_points:\n",
    "        \"\"\"\n",
    "        # ###################### prepare stages #############################\n",
    "        if not end_points:\n",
    "            end_points = {}\n",
    "        # ResNet pre + layer1 + layer2\n",
    "        rgb_emb = self.cnn_pre_stages(inputs['rgb'])  # stride = 2, [bs, c, 240, 320]\n",
    "        # rndla pre\n",
    "        xyz, p_emb = self._break_up_pc(inputs['cld_rgb_nrm'])\n",
    "        p_emb = inputs['cld_rgb_nrm']\n",
    "        p_emb = self.rndla_pre_stages(p_emb)\n",
    "        p_emb = p_emb.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###################### encoding stages #############################\n",
    "        ds_emb = []\n",
    "        for i_ds in range(4):\n",
    "            # encode rgb downsampled feature\n",
    "            rgb_emb0 = self.cnn_ds_stages[i_ds](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # encode point cloud downsampled feature\n",
    "            f_encoder_i = self.rndla_ds_stages[i_ds](\n",
    "                p_emb, inputs['cld_xyz%d' % i_ds], inputs['cld_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, inputs['cld_sub_idx%d' % i_ds])\n",
    "            p_emb0 = f_sampled_i\n",
    "            if i_ds == 0:\n",
    "                ds_emb.append(f_encoder_i)\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.ds_fuse_p2r_pre_layers[i_ds](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_ds_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.ds_fuse_p2r_fuse_layers[i_ds](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr, 1), inputs['r2p_ds_nei_idx%d' % i_ds]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.ds_fuse_r2p_pre_layers[i_ds](r2p_emb)\n",
    "            p_emb = self.ds_fuse_r2p_fuse_layers[i_ds](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "            ds_emb.append(p_emb)\n",
    "\n",
    "        # ###################### decoding stages #############################\n",
    "        n_up_layers = len(self.rndla_up_stages)\n",
    "        for i_up in range(n_up_layers-1):\n",
    "            # decode rgb upsampled feature\n",
    "            rgb_emb0 = self.cnn_up_stages[i_up](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # decode point cloud upsampled feature\n",
    "            f_interp_i = self.nearest_interpolation(\n",
    "                p_emb, inputs['cld_interp_idx%d' % (n_up_layers-i_up-1)]\n",
    "            )\n",
    "            f_decoder_i = self.rndla_up_stages[i_up](\n",
    "                torch.cat([ds_emb[-i_up - 2], f_interp_i], dim=1)\n",
    "            )\n",
    "            p_emb0 = f_decoder_i\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.up_fuse_p2r_pre_layers[i_up](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_up_nei_idx%d' % i_up]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.up_fuse_p2r_fuse_layers[i_up](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr), inputs['r2p_up_nei_idx%d' % i_up]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.up_fuse_r2p_pre_layers[i_up](r2p_emb)\n",
    "            p_emb = self.up_fuse_r2p_fuse_layers[i_up](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "\n",
    "        # final upsample layers:\n",
    "        rgb_emb = self.cnn_up_stages[n_up_layers-1](rgb_emb)\n",
    "        f_interp_i = self.nearest_interpolation(\n",
    "            p_emb, inputs['cld_interp_idx%d' % (0)]\n",
    "        )\n",
    "        p_emb = self.rndla_up_stages[n_up_layers-1](\n",
    "            torch.cat([ds_emb[0], f_interp_i], dim=1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        bs, di, _, _ = rgb_emb.size()\n",
    "        rgb_emb_c = rgb_emb.view(bs, di, -1)\n",
    "        choose_emb = inputs['choose'].repeat(1, di, 1)\n",
    "        rgb_emb_c = torch.gather(rgb_emb_c, 2, choose_emb).contiguous()\n",
    "\n",
    "        # Use DenseFusion in final layer, which will hurt performance due to overfitting\n",
    "        # rgbd_emb = self.fusion_layer(rgb_emb, pcld_emb)\n",
    "\n",
    "        # Use simple concatenation. Good enough for fully fused RGBD feature.\n",
    "        rgbd_emb = torch.cat([rgb_emb_c, p_emb], dim=1)\n",
    "\n",
    "        # ###################### prediction stages #############################\n",
    "        rgbd_segs = self.rgbd_seg_layer(rgbd_emb)\n",
    "        pred_kp_ofs = self.kp_ofst_layer(rgbd_emb)\n",
    "        pred_ctr_ofs = self.ctr_ofst_layer(rgbd_emb)\n",
    "\n",
    "        pred_kp_ofs = pred_kp_ofs.view(\n",
    "            bs, self.n_kps, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "        pred_ctr_ofs = pred_ctr_ofs.view(\n",
    "            bs, 1, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        # return rgbd_seg, pred_kp_of, pred_ctr_of\n",
    "        end_points['pred_rgbd_segs'] = rgbd_segs\n",
    "        end_points['pred_kp_ofs'] = pred_kp_ofs\n",
    "        end_points['pred_ctr_ofs'] = pred_ctr_ofs\n",
    "\n",
    "        return end_points\n",
    "\n",
    "\n",
    "# Copy from PVN3D: https://github.com/ethnhe/PVN3D\n",
    "class DenseFusion(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(DenseFusion, self).__init__()\n",
    "        self.conv2_rgb = torch.nn.Conv1d(64, 256, 1)\n",
    "        self.conv2_cld = torch.nn.Conv1d(32, 256, 1)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv1d(96, 512, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        self.ap1 = torch.nn.AvgPool1d(num_points)\n",
    "\n",
    "    def forward(self, rgb_emb, cld_emb):\n",
    "        bs, _, n_pts = cld_emb.size()\n",
    "        feat_1 = torch.cat((rgb_emb, cld_emb), dim=1)\n",
    "        rgb = F.relu(self.conv2_rgb(rgb_emb))\n",
    "        cld = F.relu(self.conv2_cld(cld_emb))\n",
    "\n",
    "        feat_2 = torch.cat((rgb, cld), dim=1)\n",
    "\n",
    "        rgbd = F.relu(self.conv3(feat_1))\n",
    "        rgbd = F.relu(self.conv4(rgbd))\n",
    "\n",
    "        ap_x = self.ap1(rgbd)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "        return torch.cat([feat_1, feat_2, ap_x], 1)  # 96+ 512 + 1024 = 1632\n",
    "\n",
    "\n",
    "def main():\n",
    "    from common import ConfigRandLA\n",
    "    rndla_cfg = ConfigRandLA\n",
    "\n",
    "    n_cls = 3\n",
    "    model = FFB6D(n_cls, rndla_cfg.num_points, rndla_cfg)\n",
    "    print(model)\n",
    "    #ffb6d_scripted =torch.jit.script(model)\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"model parameters:\", sum(param.numel() for param in model.parameters())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff792b96",
   "metadata": {},
   "source": [
    "### Config RandLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00a7c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigRandLA:\n",
    "    k_n = 16  # KNN\n",
    "    num_layers = 4  # Number of layers\n",
    "    num_points = 1920 * 1440 // 24  # Number of input points\n",
    "    num_classes = 3  # Number of valid classes\n",
    "    sub_grid_size = 0.06  # preprocess_parameter def: 0.06\n",
    "\n",
    "    batch_size = 3  # batch_size during training\n",
    "    val_batch_size = 3  # batch_size during validation and test\n",
    "    train_steps = 500  # Number of steps per epochs\n",
    "    val_steps = 100  # Number of validation steps per epoch\n",
    "    in_c = 9\n",
    "\n",
    "    sub_sampling_ratio = [4, 4, 4, 4]  # sampling ratio of random sampling at each layer\n",
    "    d_out = [32, 64, 128, 256]  # feature dimension\n",
    "    num_sub_points = [num_points // 4, num_points // 16, num_points // 64, num_points // 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38d721",
   "metadata": {},
   "source": [
    "Provide input from example data to trace the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0681c758",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e540d47d157a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import coremltools\n",
    "import tarfile\n",
    "from torch.jit import script, trace\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from torchvision import transforms\n",
    "import normalspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3a41b",
   "metadata": {},
   "source": [
    "If your Pytorch version is <1.10.2 it might not load your model correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adfd60ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Pytorch Verison\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(self, item_name):\n",
    "    with Image.open(\"data/depth/{}.png\".format(item_name)) as di:\n",
    "        dpt_mm = np.array(di)\n",
    "    with Image.open(\"data/mask/{}.png\".format(item_name)) as li:\n",
    "        labels = np.array(li)\n",
    "        labels = (labels > 0).astype(\"uint8\")\n",
    "    with Image.open(\"data/rgb/{}.png\".format(item_name)) as ri:\n",
    "        if self.add_noise:\n",
    "            ri = self.trancolor(ri)\n",
    "        rgb = np.array(ri)[:, :, :3]\n",
    "    meta_file = open('data/gt.yml', \"r\")\n",
    "    meta_lst = yaml.load(meta_file, Loader=yaml.FullLoader)\n",
    "    meta = meta_lst[int(item_name)]\n",
    "    meta = meta[0]\n",
    "    R = np.resize(np.array(meta['cam_R_m2c']), (3, 3))\n",
    "    T = np.array(meta['cam_t_m2c']) / 1000.0\n",
    "    RT = np.concatenate((R, T[:, None]), axis=1)\n",
    "    rnd_typ = 'real'\n",
    "    K = np.array([[1594.7247314453125, 0., 951.2391967773438],\n",
    "                    [0., 1594.7247314453125, 722.7899761199951],\n",
    "                    [0., 0., 1.]]),\n",
    "    cam_scale = 1000.0\n",
    "    if len(labels.shape) > 2:\n",
    "        labels = labels[:, :, 0]\n",
    "    rgb_labels = labels.copy()\n",
    "    dpt_mm = dpt_mm.copy().astype(np.uint16)\n",
    "    nrm_map = normalSpeed.depth_normal(\n",
    "        dpt_mm, K[0][0], K[1][1], 5, 2000, 20, False\n",
    "    )\n",
    "    if self.DEBUG:\n",
    "        show_nrm_map = ((nrm_map + 1.0) * 127).astype(np.uint8)\n",
    "        imN = cv2.resize(show_nrm_map, (720, 960))\n",
    "        imshow(\"nrm_map\", imN)\n",
    "\n",
    "    dpt_m = dpt_mm.astype(np.float32) / cam_scale\n",
    "    dpt_xyz = self.dpt_2_pcld(dpt_m, 1.0, K)\n",
    "    dpt_xyz[np.isnan(dpt_xyz)] = 0.0\n",
    "    dpt_xyz[np.isinf(dpt_xyz)] = 0.0\n",
    "\n",
    "    msk_dp = dpt_mm > 1e-6\n",
    "    choose = msk_dp.flatten().nonzero()[0].astype(np.uint32)\n",
    "    if len(choose) < 400:\n",
    "        return None\n",
    "    choose_2 = np.array([i for i in range(len(choose))])\n",
    "    if len(choose_2) < 400:\n",
    "        return None\n",
    "    if len(choose_2) > self.config.n_sample_points:\n",
    "        c_mask = np.zeros(len(choose_2), dtype=int)\n",
    "        c_mask[:self.config.n_sample_points] = 1\n",
    "        np.random.shuffle(c_mask)\n",
    "        choose_2 = choose_2[c_mask.nonzero()]\n",
    "    else:\n",
    "        choose_2 = np.pad(choose_2, (0, self.config.n_sample_points-len(choose_2)), 'wrap')\n",
    "    choose = np.array(choose)[choose_2]\n",
    "\n",
    "    sf_idx = np.arange(choose.shape[0])\n",
    "    np.random.shuffle(sf_idx)\n",
    "    choose = choose[sf_idx]\n",
    "\n",
    "    cld = dpt_xyz.reshape(-1, 3)[choose, :]\n",
    "    rgb_pt = rgb.reshape(-1, 3)[choose, :].astype(np.float32)\n",
    "    nrm_pt = nrm_map[:, :, :3].reshape(-1, 3)[choose, :]\n",
    "    labels_pt = labels.flatten()[choose]\n",
    "    choose = np.array([choose])\n",
    "    cld_rgb_nrm = np.concatenate((cld, rgb_pt, nrm_pt), axis=1).transpose(1, 0)\n",
    "\n",
    "    RTs, kp3ds, ctr3ds, cls_ids, kp_targ_ofst, ctr_targ_ofst = self.get_pose_gt_info(\n",
    "        cld, labels_pt, RT\n",
    "    )\n",
    "\n",
    "    h, w = rgb_labels.shape\n",
    "    dpt_6c = np.concatenate((dpt_xyz, nrm_map[:, :, :3]), axis=2).transpose(2, 0, 1)\n",
    "    rgb = np.transpose(rgb, (2, 0, 1))  # hwc2chw\n",
    "\n",
    "    xyz_lst = [dpt_xyz.transpose(2, 0, 1)]  # c, h, w\n",
    "    msk_lst = [dpt_xyz[2, :, :] > 1e-8]\n",
    "\n",
    "    for i in range(3):\n",
    "        scale = pow(2, i+1)\n",
    "        nh, nw = h // pow(2, i+1), w // pow(2, i+1)\n",
    "        ys, xs = np.mgrid[:nh, :nw]\n",
    "        xyz_lst.append(xyz_lst[0][:, ys*scale, xs*scale])\n",
    "        msk_lst.append(xyz_lst[-1][2, :, :] > 1e-8)\n",
    "    sr2dptxyz = {\n",
    "        pow(2, ii): item.reshape(3, -1).transpose(1, 0)\n",
    "        for ii, item in enumerate(xyz_lst)\n",
    "    }\n",
    "\n",
    "    rgb_ds_sr = [4, 8, 8, 8]\n",
    "    n_ds_layers = 4\n",
    "    pcld_sub_s_r = [4, 4, 4, 4]\n",
    "    inputs = {}\n",
    "    # DownSample stage\n",
    "    for i in range(n_ds_layers):\n",
    "        nei_idx = DP.knn_search(\n",
    "            cld[None, ...], cld[None, ...], 16\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        sub_pts = cld[:cld.shape[0] // pcld_sub_s_r[i], :]\n",
    "        pool_i = nei_idx[:cld.shape[0] // pcld_sub_s_r[i], :]\n",
    "        up_i = DP.knn_search(\n",
    "            sub_pts[None, ...], cld[None, ...], 1\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        inputs['cld_xyz%d' % i] = cld.astype(np.float32).copy()\n",
    "        inputs['cld_nei_idx%d' % i] = nei_idx.astype(np.int32).copy()\n",
    "        inputs['cld_sub_idx%d' % i] = pool_i.astype(np.int32).copy()\n",
    "        inputs['cld_interp_idx%d' % i] = up_i.astype(np.int32).copy()\n",
    "        nei_r2p = DP.knn_search(\n",
    "            sr2dptxyz[rgb_ds_sr[i]][None, ...], sub_pts[None, ...], 16\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        inputs['r2p_ds_nei_idx%d' % i] = nei_r2p.copy()\n",
    "        nei_p2r = DP.knn_search(\n",
    "            sub_pts[None, ...], sr2dptxyz[rgb_ds_sr[i]][None, ...], 1\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        inputs['p2r_ds_nei_idx%d' % i] = nei_p2r.copy()\n",
    "        cld = sub_pts\n",
    "\n",
    "    n_up_layers = 3\n",
    "    rgb_up_sr = [4, 2, 2]\n",
    "    for i in range(n_up_layers):\n",
    "        r2p_nei = DP.knn_search(\n",
    "            sr2dptxyz[rgb_up_sr[i]][None, ...],\n",
    "            inputs['cld_xyz%d'%(n_ds_layers-i-1)][None, ...], 16\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        inputs['r2p_up_nei_idx%d' % i] = r2p_nei.copy()\n",
    "        p2r_nei = DP.knn_search(\n",
    "            inputs['cld_xyz%d'%(n_ds_layers-i-1)][None, ...],\n",
    "            sr2dptxyz[rgb_up_sr[i]][None, ...], 1\n",
    "        ).astype(np.int32).squeeze(0)\n",
    "        inputs['p2r_up_nei_idx%d' % i] = p2r_nei.copy()\n",
    "\n",
    "    show_rgb = rgb.transpose(1, 2, 0).copy()[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b7d1e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-85a06b78ac98>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-85a06b78ac98>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    example_input =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=m, std=s),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "\n",
    "example_input = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a20d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = \"vase\"\n",
    "model_pth = \"model/FFB6D_{}_best.pth.tar\".format(cls)\n",
    "\n",
    "# Create an instance of your pretrained model.\n",
    "try:\n",
    "    model = torch.load(model_pth, map_location=torch.device('cpu'))\n",
    "except Exception:\n",
    "    model = pkl.load(open(model_pth, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ae470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine input shape of network\n",
    "shape_of_first_layer = list(model.parameters())[0].shape #shape_of_first_layer\n",
    "\n",
    "N,C = shape_of_first_layer[:2]\n",
    "\n",
    "dummy_input = torch.Tensor(N,C)\n",
    "\n",
    "dummy_input = dummy_input[...,:, 1920,1440] #adding the None for height and weight\n",
    "\n",
    "torch.onnx.export(net, dummy_input, './alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1446d65",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unable to infer type of dictionary: Dictionary inputs to traced functions must have consistent type. Found int and float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-084e35b987f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use torch.jit.script to generate a torch.jit.ScriptModule via scripting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscripted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check the TS graph, for further debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscripted_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter-ffb6d/lib/python3.6/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_script_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter-ffb6d/lib/python3.6/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mcreate_script_dict\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mzero\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0moverhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \"\"\"\n\u001b[0;32m--> 993\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Unable to infer type of dictionary: Dictionary inputs to traced functions must have consistent type. Found int and float"
     ]
    }
   ],
   "source": [
    "# Use torch.jit.script to generate a torch.jit.ScriptModule via scripting.\n",
    "scripted_model = torch.jit.script(model, sample_input)\n",
    "\n",
    "# check the TS graph, for further debugging\n",
    "print(scripted_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save(\"unet.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41322863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke CoreML Converter\n",
    "mlmodel = coremltools.converters.convert(\n",
    "  scripted_model,\n",
    "  inputs=[coremltools.TensorType(shape=(1, 3, 64, 64))],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
