{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248291c5",
   "metadata": {},
   "source": [
    "# Convert FFB6D to TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db67a3",
   "metadata": {},
   "source": [
    "After cloning this repositiory please follow the instructions on compiling apex, normalspeed and RandLA.\n",
    "After you obtained a FFB6D model, you can follow these steps to convert it into Torchschript:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23846",
   "metadata": {},
   "source": [
    "# Add configuration for RandLAN & PSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407c0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.cnn.pspnet import PSPNet\n",
    "import models.pytorch_utils as pt_utils\n",
    "from models.RandLA.RandLANet import Network as RandLANet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69381237",
   "metadata": {},
   "source": [
    "## Some pre-defined Torch Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d8846",
   "metadata": {},
   "source": [
    "## Add PSPNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7929ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit import script, trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super(PSPModule, self).__init__()\n",
    "        self.stages = []\n",
    "        self.stages = nn.ModuleList(\n",
    "            [self._make_stage(features, size) for size in sizes]\n",
    "        )\n",
    "        self.bottleneck = nn.Conv2d(\n",
    "            features * (len(sizes) + 1), out_features, kernel_size=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)\n",
    "        priors = [\n",
    "            F.upsample(input=stage(feats), size=(h, w), mode='bilinear')\n",
    "            for stage in self.stages\n",
    "        ] + [feats]\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)\n",
    "\n",
    "\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PSPUpsample, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Modified_PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "                 deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "                 ):\n",
    "        super(Modified_PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_classes=22, sizes=(1, 2, 3, 6), psp_size=2048,\n",
    "            deep_features_size=1024, backend='resnet18', pretrained=True\n",
    "    ):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
    "        self.final = nn.Sequential(\n",
    "            # nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f, class_f = self.feats(x)\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)\n",
    "\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        return self.final(p), self.final_seg(p).permute(0, 2, 3, 1).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da0631",
   "metadata": {},
   "source": [
    "## Add RandLANet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.fc0 = pt_utils.Conv1d(config.in_c, 8, kernel_size=1, bn=True)\n",
    "\n",
    "        self.dilated_res_blocks = nn.ModuleList()\n",
    "        d_in = 8\n",
    "        for i in range(self.config.num_layers):\n",
    "            d_out = self.config.d_out[i]\n",
    "            self.dilated_res_blocks.append(Dilated_res_block(d_in, d_out))\n",
    "            d_in = 2 * d_out\n",
    "\n",
    "        d_out = d_in\n",
    "        self.decoder_0 = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        for j in range(self.config.num_layers):\n",
    "            if j < 3:\n",
    "                d_in = d_out + 2 * self.config.d_out[-j-2]\n",
    "                d_out = 2 * self.config.d_out[-j-2]\n",
    "            else:\n",
    "                d_in = 4 * self.config.d_out[-4]\n",
    "                d_out = 2 * self.config.d_out[-4]\n",
    "            self.decoder_blocks.append(pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True))\n",
    "\n",
    "        self.fc1 = pt_utils.Conv2d(d_out, 64, kernel_size=(1,1), bn=True)\n",
    "        self.fc2 = pt_utils.Conv2d(64, 32, kernel_size=(1,1), bn=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = pt_utils.Conv2d(32, self.config.num_classes, kernel_size=(1,1), bn=False, activation=None)\n",
    "\n",
    "    def forward(self, end_points):\n",
    "\n",
    "        features = end_points['features']  # Batch*channel*npoints\n",
    "        features = self.fc0(features)\n",
    "\n",
    "        features = features.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###########################Encoder############################\n",
    "        f_encoder_list = []\n",
    "        for i in range(self.config.num_layers):\n",
    "            f_encoder_i = self.dilated_res_blocks[i](\n",
    "                features, end_points['xyz'][i], end_points['neigh_idx'][i]\n",
    "            )\n",
    "\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, end_points['sub_idx'][i])\n",
    "            features = f_sampled_i\n",
    "            print(\"encoder%d:\"%i, features.size())\n",
    "            if i == 0:\n",
    "                f_encoder_list.append(f_encoder_i)\n",
    "            f_encoder_list.append(f_sampled_i)\n",
    "        # ###########################Encoder############################\n",
    "\n",
    "        features = self.decoder_0(f_encoder_list[-1])\n",
    "\n",
    "        # ###########################Decoder############################\n",
    "        f_decoder_list = []\n",
    "        for j in range(self.config.num_layers):\n",
    "            f_interp_i = self.nearest_interpolation(features, end_points['interp_idx'][-j - 1])\n",
    "            f_decoder_i = self.decoder_blocks[j](torch.cat([f_encoder_list[-j - 2], f_interp_i], dim=1))\n",
    "\n",
    "            features = f_decoder_i\n",
    "            print(\"decoder%d:\"%j, features.size())\n",
    "            f_decoder_list.append(f_decoder_i)\n",
    "        # ###########################Decoder############################\n",
    "\n",
    "        features = self.fc1(features)\n",
    "        features = self.fc2(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc3(features)\n",
    "        f_out = features.squeeze(3)\n",
    "\n",
    "        end_points['logits'] = f_out\n",
    "        return end_points\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1))\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, C, npoints] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, c, up_num_points, 1] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(feature, 2, interp_idx.unsqueeze(1).repeat(1,feature.shape[1],1))\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "\n",
    "\n",
    "def compute_acc(end_points):\n",
    "\n",
    "    logits = end_points['valid_logits']\n",
    "    labels = end_points['valid_labels']\n",
    "    logits = logits.max(dim=1)[1]\n",
    "    acc = (logits == labels).sum().float() / float(labels.shape[0])\n",
    "    end_points['acc'] = acc\n",
    "    return acc, end_points\n",
    "\n",
    "\n",
    "class IoUCalculator:\n",
    "    def __init__(self, cfg):\n",
    "        self.gt_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.true_positive_classes = [0 for _ in range(cfg.num_classes)]\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def add_data(self, end_points):\n",
    "        logits = end_points['valid_logits']\n",
    "        labels = end_points['valid_labels']\n",
    "        pred = logits.max(dim=1)[1]\n",
    "        pred_valid = pred.detach().cpu().numpy()\n",
    "        labels_valid = labels.detach().cpu().numpy()\n",
    "\n",
    "        val_total_correct = 0\n",
    "        val_total_seen = 0\n",
    "\n",
    "        correct = np.sum(pred_valid == labels_valid)\n",
    "        val_total_correct += correct\n",
    "        val_total_seen += len(labels_valid)\n",
    "\n",
    "        conf_matrix = confusion_matrix(labels_valid, pred_valid, np.arange(0, self.cfg.num_classes, 1))\n",
    "        self.gt_classes += np.sum(conf_matrix, axis=1)\n",
    "        self.positive_classes += np.sum(conf_matrix, axis=0)\n",
    "        self.true_positive_classes += np.diagonal(conf_matrix)\n",
    "\n",
    "    def compute_iou(self):\n",
    "        iou_list = []\n",
    "        for n in range(0, self.cfg.num_classes, 1):\n",
    "            if float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n]) != 0:\n",
    "                iou = self.true_positive_classes[n] / float(self.gt_classes[n] + self.positive_classes[n] - self.true_positive_classes[n])\n",
    "                iou_list.append(iou)\n",
    "            else:\n",
    "                iou_list.append(0.0)\n",
    "        mean_iou = sum(iou_list) / float(self.cfg.num_classes)\n",
    "        return mean_iou, iou_list\n",
    "\n",
    "\n",
    "\n",
    "class Dilated_res_block(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = pt_utils.Conv2d(d_in, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.lfa = Building_block(d_out)\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out, d_out*2, kernel_size=(1, 1), bn=True, activation=None)\n",
    "        self.shortcut = pt_utils.Conv2d(d_in, d_out*2, kernel_size=(1,1), bn=True, activation=None)\n",
    "\n",
    "    def forward(self, feature, xyz, neigh_idx):\n",
    "        f_pc = self.mlp1(feature)  # Batch*channel*npoints*1\n",
    "        f_pc = self.lfa(xyz, f_pc, neigh_idx)  # Batch*d_out*npoints*1\n",
    "        f_pc = self.mlp2(f_pc)\n",
    "        shortcut = self.shortcut(feature)\n",
    "        return F.leaky_relu(f_pc+shortcut, negative_slope=0.2)\n",
    "\n",
    "\n",
    "class Building_block(nn.Module):\n",
    "    def __init__(self, d_out):  #  d_in = d_out//2\n",
    "        super().__init__()\n",
    "        self.mlp1 = pt_utils.Conv2d(10, d_out//2, kernel_size=(1,1), bn=True)\n",
    "        self.att_pooling_1 = Att_pooling(d_out, d_out//2)\n",
    "\n",
    "        self.mlp2 = pt_utils.Conv2d(d_out//2, d_out//2, kernel_size=(1, 1), bn=True)\n",
    "        self.att_pooling_2 = Att_pooling(d_out, d_out)\n",
    "\n",
    "    def forward(self, xyz, feature, neigh_idx):  # feature: Batch*channel*npoints*1\n",
    "        f_xyz = self.relative_pos_encoding(xyz, neigh_idx)  # batch*npoint*nsamples*10\n",
    "        f_xyz = f_xyz.permute((0, 3, 1, 2)).contiguous()  # batch*10*npoint*nsamples\n",
    "        f_xyz = self.mlp1(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            feature.squeeze(-1).permute((0, 2, 1)).contiguous(),neigh_idx\n",
    "        )  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_1(f_concat)  # Batch*channel*npoints*1\n",
    "\n",
    "        f_xyz = self.mlp2(f_xyz)\n",
    "        f_neighbours = self.gather_neighbour(\n",
    "            f_pc_agg.squeeze(-1).permute((0, 2, 1)).contiguous(), neigh_idx\n",
    "        ).contiguous()  # batch*npoint*nsamples*channel\n",
    "        f_neighbours = f_neighbours.permute((0, 3, 1, 2)).contiguous()  # batch*channel*npoint*nsamples\n",
    "        f_concat = torch.cat([f_neighbours, f_xyz], dim=1)\n",
    "        f_pc_agg = self.att_pooling_2(f_concat)\n",
    "        return f_pc_agg\n",
    "\n",
    "    def relative_pos_encoding(self, xyz, neigh_idx):\n",
    "        neighbor_xyz = self.gather_neighbour(xyz, neigh_idx)  # batch*npoint*nsamples*3\n",
    "\n",
    "        xyz_tile = xyz.unsqueeze(2).repeat(1, 1, neigh_idx.shape[-1], 1)  # batch*npoint*nsamples*3\n",
    "        relative_xyz = xyz_tile - neighbor_xyz  # batch*npoint*nsamples*3\n",
    "        relative_dis = torch.sqrt(torch.sum(torch.pow(relative_xyz, 2), dim=-1, keepdim=True))  # batch*npoint*nsamples*1\n",
    "        relative_feature = torch.cat([relative_dis, relative_xyz, xyz_tile, neighbor_xyz], dim=-1)  # batch*npoint*nsamples*10\n",
    "        return relative_feature\n",
    "\n",
    "    @staticmethod\n",
    "    def gather_neighbour(pc, neighbor_idx):  # pc: batch*npoint*channel\n",
    "        # gather the coordinates or features of neighboring points\n",
    "        batch_size = pc.shape[0]\n",
    "        num_points = pc.shape[1]\n",
    "        d = pc.shape[2]\n",
    "        index_input = neighbor_idx.reshape(batch_size, -1)\n",
    "        features = torch.gather(pc, 1, index_input.unsqueeze(-1).repeat(1, 1, pc.shape[2])).contiguous()\n",
    "        features = features.reshape(batch_size, num_points, neighbor_idx.shape[-1], d)  # batch*npoint*nsamples*channel\n",
    "        return features\n",
    "\n",
    "\n",
    "class Att_pooling(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Conv2d(d_in, d_in, (1, 1), bias=False)\n",
    "        self.mlp = pt_utils.Conv2d(d_in, d_out, kernel_size=(1,1), bn=True)\n",
    "\n",
    "    def forward(self, feature_set):\n",
    "\n",
    "        att_activation = self.fc(feature_set)\n",
    "        att_scores = F.softmax(att_activation, dim=3)\n",
    "        f_agg = feature_set * att_scores\n",
    "        f_agg = torch.sum(f_agg, dim=3, keepdim=True)\n",
    "        f_agg = self.mlp(f_agg)\n",
    "        return f_agg\n",
    "\n",
    "\n",
    "def compute_loss(end_points, cfg):\n",
    "\n",
    "    logits = end_points['logits']\n",
    "    labels = end_points['labels']\n",
    "\n",
    "    logits = logits.transpose(1, 2).reshape(-1, cfg.num_classes)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    # Boolean mask of points that should be ignored\n",
    "    ignored_bool = labels == 0\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        ignored_bool = ignored_bool | (labels == ign_label)\n",
    "\n",
    "    # Collect logits and labels that are not ignored\n",
    "    valid_idx = ignored_bool == 0\n",
    "    valid_logits = logits[valid_idx, :]\n",
    "    valid_labels_init = labels[valid_idx]\n",
    "\n",
    "    # Reduce label values in the range of logit shape\n",
    "    reducing_list = torch.range(0, cfg.num_classes).long().cuda()\n",
    "    inserted_value = torch.zeros((1,)).long().cuda()\n",
    "    for ign_label in cfg.ignored_label_inds:\n",
    "        reducing_list = torch.cat([reducing_list[:ign_label], inserted_value, reducing_list[ign_label:]], 0)\n",
    "    valid_labels = torch.gather(reducing_list, 0, valid_labels_init)\n",
    "    loss = get_loss(valid_logits, valid_labels, cfg.class_weights)\n",
    "    end_points['valid_logits'], end_points['valid_labels'] = valid_logits, valid_labels\n",
    "    end_points['loss'] = loss\n",
    "    return loss, end_points\n",
    "\n",
    "\n",
    "def get_loss(logits, labels, pre_cal_weights):\n",
    "    # calculate the weighted cross entropy according to the inverse frequency\n",
    "    class_weights = torch.from_numpy(pre_cal_weights).float().cuda()\n",
    "    # one_hot_labels = F.one_hot(labels, self.config.num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='none')\n",
    "    output_loss = criterion(logits, labels)\n",
    "    output_loss = output_loss.mean()\n",
    "    return output_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53764e",
   "metadata": {},
   "source": [
    "# FFB6D\n",
    "\n",
    "Finally add FFB6D and trace the model flow with TorchJitScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c914164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.cnn.pspnet import PSPNet\n",
    "import models.pytorch_utils as pt_utils\n",
    "from models.RandLA.RandLANet import Network as RandLANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "}\n",
    "\n",
    "\n",
    "class FFB6D(nn.Module):\n",
    "    def __init__(self, n_classes, n_pts, rndla_cfg, n_kps=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # ######################## prepare stages#########################\n",
    "        self.n_cls = n_classes\n",
    "        self.n_pts = n_pts\n",
    "        self.n_kps = n_kps\n",
    "        cnn = psp_models['resnet34'.lower()]()\n",
    "\n",
    "        rndla = RandLANet(rndla_cfg)\n",
    "\n",
    "        self.cnn_pre_stages = nn.Sequential (\n",
    "            cnn.feats.conv1,  # stride = 2, [bs, c, 240, 320]\n",
    "            cnn.feats.bn1, cnn.feats.relu,\n",
    "            cnn.feats.maxpool  # stride = 2, [bs, 64, 120, 160]\n",
    "        )\n",
    "        self.rndla_pre_stages = rndla.fc0\n",
    "\n",
    "        # ####################### downsample stages#######################\n",
    "        self.cnn_ds_stages = nn.ModuleList([\n",
    "            cnn.feats.layer1,    # stride = 1, [bs, 64, 120, 160]\n",
    "            cnn.feats.layer2,    # stride = 2, [bs, 128, 60, 80]\n",
    "            # stride = 1, [bs, 128, 60, 80]\n",
    "            nn.Sequential(cnn.feats.layer3, cnn.feats.layer4),\n",
    "            nn.Sequential(cnn.psp, cnn.drop_1)   # [bs, 1024, 60, 80]\n",
    "        ])\n",
    "        self.ds_sr = [4, 8, 8, 8]\n",
    "\n",
    "        self.rndla_ds_stages = rndla.dilated_res_blocks\n",
    "\n",
    "        self.ds_rgb_oc = [64, 128, 512, 1024]\n",
    "        self.ds_rndla_oc = [item * 2 for item in rndla_cfg.d_out]\n",
    "        self.ds_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.ds_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.ds_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i], self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i]*2, self.ds_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.ds_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rndla_oc[i], self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.ds_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.ds_rgb_oc[i]*2, self.ds_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ###################### upsample stages #############################\n",
    "        self.cnn_up_stages = nn.ModuleList([\n",
    "            nn.Sequential(cnn.up_1, cnn.drop_2),  # [bs, 256, 120, 160]\n",
    "            nn.Sequential(cnn.up_2, cnn.drop_2),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.final),  # [bs, 64, 240, 320]\n",
    "            nn.Sequential(cnn.up_3, cnn.final)  # [bs, 64, 480, 640]\n",
    "        ])\n",
    "        self.up_rgb_oc = [256, 64, 64]\n",
    "        self.up_rndla_oc = []\n",
    "        for j in range(rndla_cfg.num_layers):\n",
    "            if j < 3:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[-j-2])\n",
    "            else:\n",
    "                self.up_rndla_oc.append(self.ds_rndla_oc[0])\n",
    "\n",
    "        self.rndla_up_stages = rndla.decoder_blocks\n",
    "\n",
    "        n_fuse_layer = 3\n",
    "        self.up_fuse_r2p_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_r2p_fuse_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_pre_layers = nn.ModuleList()\n",
    "        self.up_fuse_p2r_fuse_layers = nn.ModuleList()\n",
    "        for i in range(n_fuse_layer):\n",
    "            self.up_fuse_r2p_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i], self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_r2p_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i]*2, self.up_rndla_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.up_fuse_p2r_pre_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rndla_oc[i], self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "            self.up_fuse_p2r_fuse_layers.append(\n",
    "                pt_utils.Conv2d(\n",
    "                    self.up_rgb_oc[i]*2, self.up_rgb_oc[i], kernel_size=(1, 1),\n",
    "                    bn=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ####################### prediction headers #############################\n",
    "        # We use 3D keypoint prediction header for pose estimation following PVN3D\n",
    "        # You can use different prediction headers for different downstream tasks.\n",
    "\n",
    "        self.rgbd_seg_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1] + self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_classes, activation=None)\n",
    "        )\n",
    "\n",
    "        self.ctr_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(3, activation=None)\n",
    "        )\n",
    "\n",
    "        self.kp_ofst_layer = (\n",
    "            pt_utils.Seq(self.up_rndla_oc[-1]+self.up_rgb_oc[-1])\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(128, bn=True, activation=nn.ReLU())\n",
    "            .conv1d(n_kps*3, activation=None)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        if len(feature.size()) > 3:\n",
    "            feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        num_neigh = pool_idx.shape[-1]\n",
    "        d = feature.shape[1]\n",
    "        batch_size = pool_idx.shape[0]\n",
    "        pool_idx = pool_idx.reshape(batch_size, -1)  # batch*(npoints,nsamples)\n",
    "        pool_features = torch.gather(\n",
    "            feature, 2, pool_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        pool_features = pool_features.reshape(batch_size, d, -1, num_neigh)\n",
    "        pool_features = pool_features.max(dim=3, keepdim=True)[0]  # batch*channel*npoints*1\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, up_num_points, d] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = feature.squeeze(dim=3)  # batch*channel*npoints\n",
    "        batch_size = interp_idx.shape[0]\n",
    "        up_num_points = interp_idx.shape[1]\n",
    "        interp_idx = interp_idx.reshape(batch_size, up_num_points)\n",
    "        interpolated_features = torch.gather(\n",
    "            feature, 2, interp_idx.unsqueeze(1).repeat(1, feature.shape[1], 1)\n",
    "        ).contiguous()\n",
    "        interpolated_features = interpolated_features.unsqueeze(3)  # batch*channel*npoints*1\n",
    "        return interpolated_features\n",
    "\n",
    "    def _break_up_pc(self, pc):\n",
    "        xyz = pc[:, :3, :].transpose(1, 2).contiguous()\n",
    "        features = (\n",
    "            pc[:, 3:, :].contiguous() if pc.size(1) > 3 else None\n",
    "        )\n",
    "        return xyz, features\n",
    "\n",
    "    def forward(\n",
    "        self, inputs, end_points=None, scale=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        inputs: dict of :\n",
    "            rgb         : FloatTensor [bs, 3, h, w]\n",
    "            dpt_nrm     : FloatTensor [bs, 6, h, w], 3c xyz in meter + 3c normal map\n",
    "            cld_rgb_nrm : FloatTensor [bs, 9, npts]\n",
    "            choose      : LongTensor [bs, 1, npts]\n",
    "            xmap, ymap: [bs, h, w]\n",
    "            K:          [bs, 3, 3]\n",
    "        Returns:\n",
    "            end_points:\n",
    "        \"\"\"\n",
    "        # ###################### prepare stages #############################\n",
    "        if not end_points:\n",
    "            end_points = {}\n",
    "        # ResNet pre + layer1 + layer2\n",
    "        rgb_emb = self.cnn_pre_stages(inputs['rgb'])  # stride = 2, [bs, c, 240, 320]\n",
    "        # rndla pre\n",
    "        xyz, p_emb = self._break_up_pc(inputs['cld_rgb_nrm'])\n",
    "        p_emb = inputs['cld_rgb_nrm']\n",
    "        p_emb = self.rndla_pre_stages(p_emb)\n",
    "        p_emb = p_emb.unsqueeze(dim=3)  # Batch*channel*npoints*1\n",
    "\n",
    "        # ###################### encoding stages #############################\n",
    "        ds_emb = []\n",
    "        for i_ds in range(4):\n",
    "            # encode rgb downsampled feature\n",
    "            rgb_emb0 = self.cnn_ds_stages[i_ds](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # encode point cloud downsampled feature\n",
    "            f_encoder_i = self.rndla_ds_stages[i_ds](\n",
    "                p_emb, inputs['cld_xyz%d' % i_ds], inputs['cld_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, inputs['cld_sub_idx%d' % i_ds])\n",
    "            p_emb0 = f_sampled_i\n",
    "            if i_ds == 0:\n",
    "                ds_emb.append(f_encoder_i)\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.ds_fuse_p2r_pre_layers[i_ds](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_ds_nei_idx%d' % i_ds]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.ds_fuse_p2r_fuse_layers[i_ds](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr, 1), inputs['r2p_ds_nei_idx%d' % i_ds]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.ds_fuse_r2p_pre_layers[i_ds](r2p_emb)\n",
    "            p_emb = self.ds_fuse_r2p_fuse_layers[i_ds](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "            ds_emb.append(p_emb)\n",
    "\n",
    "        # ###################### decoding stages #############################\n",
    "        n_up_layers = len(self.rndla_up_stages)\n",
    "        for i_up in range(n_up_layers-1):\n",
    "            # decode rgb upsampled feature\n",
    "            rgb_emb0 = self.cnn_up_stages[i_up](rgb_emb)\n",
    "            bs, c, hr, wr = rgb_emb0.size()\n",
    "\n",
    "            # decode point cloud upsampled feature\n",
    "            f_interp_i = self.nearest_interpolation(\n",
    "                p_emb, inputs['cld_interp_idx%d' % (n_up_layers-i_up-1)]\n",
    "            )\n",
    "            f_decoder_i = self.rndla_up_stages[i_up](\n",
    "                torch.cat([ds_emb[-i_up - 2], f_interp_i], dim=1)\n",
    "            )\n",
    "            p_emb0 = f_decoder_i\n",
    "\n",
    "            # fuse point feauture to rgb feature\n",
    "            p2r_emb = self.up_fuse_p2r_pre_layers[i_up](p_emb0)\n",
    "            p2r_emb = self.nearest_interpolation(\n",
    "                p2r_emb, inputs['p2r_up_nei_idx%d' % i_up]\n",
    "            )\n",
    "            p2r_emb = p2r_emb.view(bs, -1, hr, wr)\n",
    "            rgb_emb = self.up_fuse_p2r_fuse_layers[i_up](\n",
    "                torch.cat((rgb_emb0, p2r_emb), dim=1)\n",
    "            )\n",
    "\n",
    "            # fuse rgb feature to point feature\n",
    "            r2p_emb = self.random_sample(\n",
    "                rgb_emb0.reshape(bs, c, hr*wr), inputs['r2p_up_nei_idx%d' % i_up]\n",
    "            ).view(bs, c, -1, 1)\n",
    "            r2p_emb = self.up_fuse_r2p_pre_layers[i_up](r2p_emb)\n",
    "            p_emb = self.up_fuse_r2p_fuse_layers[i_up](\n",
    "                torch.cat((p_emb0, r2p_emb), dim=1)\n",
    "            )\n",
    "\n",
    "        # final upsample layers:\n",
    "        rgb_emb = self.cnn_up_stages[n_up_layers-1](rgb_emb)\n",
    "        f_interp_i = self.nearest_interpolation(\n",
    "            p_emb, inputs['cld_interp_idx%d' % (0)]\n",
    "        )\n",
    "        p_emb = self.rndla_up_stages[n_up_layers-1](\n",
    "            torch.cat([ds_emb[0], f_interp_i], dim=1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        bs, di, _, _ = rgb_emb.size()\n",
    "        rgb_emb_c = rgb_emb.view(bs, di, -1)\n",
    "        choose_emb = inputs['choose'].repeat(1, di, 1)\n",
    "        rgb_emb_c = torch.gather(rgb_emb_c, 2, choose_emb).contiguous()\n",
    "\n",
    "        # Use DenseFusion in final layer, which will hurt performance due to overfitting\n",
    "        # rgbd_emb = self.fusion_layer(rgb_emb, pcld_emb)\n",
    "\n",
    "        # Use simple concatenation. Good enough for fully fused RGBD feature.\n",
    "        rgbd_emb = torch.cat([rgb_emb_c, p_emb], dim=1)\n",
    "\n",
    "        # ###################### prediction stages #############################\n",
    "        rgbd_segs = self.rgbd_seg_layer(rgbd_emb)\n",
    "        pred_kp_ofs = self.kp_ofst_layer(rgbd_emb)\n",
    "        pred_ctr_ofs = self.ctr_ofst_layer(rgbd_emb)\n",
    "\n",
    "        pred_kp_ofs = pred_kp_ofs.view(\n",
    "            bs, self.n_kps, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "        pred_ctr_ofs = pred_ctr_ofs.view(\n",
    "            bs, 1, 3, -1\n",
    "        ).permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        # return rgbd_seg, pred_kp_of, pred_ctr_of\n",
    "        end_points['pred_rgbd_segs'] = rgbd_segs\n",
    "        end_points['pred_kp_ofs'] = pred_kp_ofs\n",
    "        end_points['pred_ctr_ofs'] = pred_ctr_ofs\n",
    "\n",
    "        return end_points\n",
    "\n",
    "\n",
    "# Copy from PVN3D: https://github.com/ethnhe/PVN3D\n",
    "class DenseFusion(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(DenseFusion, self).__init__()\n",
    "        self.conv2_rgb = torch.nn.Conv1d(64, 256, 1)\n",
    "        self.conv2_cld = torch.nn.Conv1d(32, 256, 1)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv1d(96, 512, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        self.ap1 = torch.nn.AvgPool1d(num_points)\n",
    "\n",
    "    def forward(self, rgb_emb, cld_emb):\n",
    "        bs, _, n_pts = cld_emb.size()\n",
    "        feat_1 = torch.cat((rgb_emb, cld_emb), dim=1)\n",
    "        rgb = F.relu(self.conv2_rgb(rgb_emb))\n",
    "        cld = F.relu(self.conv2_cld(cld_emb))\n",
    "\n",
    "        feat_2 = torch.cat((rgb, cld), dim=1)\n",
    "\n",
    "        rgbd = F.relu(self.conv3(feat_1))\n",
    "        rgbd = F.relu(self.conv4(rgbd))\n",
    "\n",
    "        ap_x = self.ap1(rgbd)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "        return torch.cat([feat_1, feat_2, ap_x], 1)  # 96+ 512 + 1024 = 1632\n",
    "\n",
    "\n",
    "def main():\n",
    "    from common import ConfigRandLA\n",
    "    rndla_cfg = ConfigRandLA\n",
    "\n",
    "    n_cls = 3\n",
    "    model = FFB6D(n_cls, rndla_cfg.num_points, rndla_cfg)\n",
    "    print(model)\n",
    "    #ffb6d_scripted =torch.jit.script(model)\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"model parameters:\", sum(param.numel() for param in model.parameters())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38d721",
   "metadata": {},
   "source": [
    "Provide input from example data to trace the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0681c758",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coremltools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5bf9ddba4efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcoremltools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coremltools'"
     ]
    }
   ],
   "source": [
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ControlFlowNet(num_channels=3)\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "mlmodel = coremltools.converters.convert(\n",
    "  scripted_model,\n",
    "  inputs=[coremltools.TensorType(shape=(1, 3, 64, 64))],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
